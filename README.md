# devmap

# Организация разработки
   ### Система
   * [на каких ОС работал](#на-каких-ОС-работал-)
   * [базовые знания `UNIX`](#базовые-знания-UNIX-)
   * Основы работы с файлами
   * [Как найти файл с определенной подстрокой в директории со вложенностями?](#Как-найти-файл-с-определенной-подстрокой-в-директории-со-вложенностями?-)
   ### Система контроля версий
   * [Знакомы ли c `Source code management (SCM)` системами - `Git`, `Mercurial`, `SVN`, etc?](#Знакомы-ли-c-SCM-системами---Git,-Mercurial,-SVN,-etc?-)
   * [зачем нужны `Source code management (SCM)`](#зачем-нужна-)
   * [какими пользовались](#какими-пользовались-)
   ### Git
   * [`gitflow`, `cherypick`, `revert`](#gitflow,-cherypick-)
   * [как перенести изменения из одной ветку в другую (2 способа)](#как-перенести-изменения-из-одной-ветку-в-другую-(2-способа)-)
   * [зачем нужна команда `git rebase`](#зачем-нужна-команда-git-rebase-)
   * [`rebase`, `merge`, `stash`](#Чем-отличается-rebase-от-merge-)
   * [разница между `git` и `svn` (если есть)](#разница-между-git-и-svn-(если-есть)-)
   ### Система тикетов и организация задач
   * [зачем нужна](#зачем-нужна-)
   * [какими пользовались](#какими-пользовались-)
   * [как была организована работа в команде](#как-была-организована-работа-в-команде-)
   * [методы разрешения конфликтов](#методы-разрешения-конфликтов-)
   * [понимание методологии `Scrum` и ее атрибутов (стендапы, грумминг/оценка бэклога, ретроспективы)](#понимание-методологии-Scrum-и-ее-атрибутов-(стендапы,-грумминг/оценка-бэклога,-ретроспективы)-)


# Кэширование
* [какие виды кэширования использовал](#какие-виды-кэширования-использовал-)
* [`Memcache`, `Redis`](#Memcache,-Redis-)
* [Если использовал `Memcache`, то с какими проблемами сталкивался.](#Если-использовал-Memcache,-то-с-какими-проблемами-сталкивался.-)

# Тесты
* [зачем нужны](#зачем-нужны-)
* [функциональные, стресс, юнит тесты](#функциональные,-стресс,-юнит-тесты-)
* [что такое `TDD`](#что-такое-TDD-)


# Web специфика
* [виды сетевых протоколов](#виды-сетевых-протоколов-)
* [Что такое `https` и зачем он нужен](#Что-такое-https-и-зачем-он-нужен-)
* [Понимание базовых аспектов функционирования сети - протоколы, `DNS`, и т.д.;](#Понимание-базовых-аспектов-функционирования-сети---протоколы,-DNS,-и-т.д.;-)
* [Что происходит при открытии вкладки браузера](#Что-происходит-при-открытии-вкладки-браузера-)
* [C каким серверным ПО приходилось работать?](#C-каким-серверным-ПО-приходилось-работать?-)
* [Что такое `Apache` и `mod_rewrite`?](#Что-такое-Apache-и-mod_rewrite?-)
* [`nginx`, его отличие от `apache`](#nginx,-его-отличие-от-apache-)
* [балансировка нагрузки на сервера приложений (`haproxy`)](#балансировка-нагрузки-на-сервера-приложений-(haproxy)-)
* [работы с системами очередей (`RabbitMQ`, `Kafka`)](#работы-с-системами-очередей-(RabbitMQ,-Kafka)-)
* [`CI` (`Continuous Integration`)](#CI-(Continuous-Integration)-)
* [Деплой](#Деплой-)
* [`Composer`](#Composer-)
* [`docker`](#docker-)
* [Стандарты написания кода](#Стандарты-написания-кода-)
* [Шаги по оптимизации сайта](#Шаги-по-оптимизации-сайта-)


# Разработка
* [Интересный крон (запуск скрипта раз в 30 секунд)](#Интересный-крон-(запуск-скрипта-раз-в-30-секунд)-)
* [Защита от спама - предложить интересный способ](#Защита-от-спама---предложить-интересный-способ-)
* [Защита от повторной отправки форм](#Защита-от-повторной-отправки-форм-)
* [Сортировка пузырьком, сложность алгоритма](#Сортировка-пузырьком,-сложность-алгоритма-)
* [Веб-сервисы, отличия, когда что использовать (`soap`, архитектура `rest`)](#Веб-сервисы,-отличия,-когда-что-использовать-(soap,-архитектура-rest)-)
* [`raise condition`](#raise-condition-)
* [`CORS`](#CORS-)
* [`SPINX`](#SPINX-)
* [гексагональная архитектура](#гексагональная-архитектура-)
* [`reflection API`](#reflection-API-)
* [`SPL`](#SPL-)
* [`SOLID`](#SOLID-)
* [`DRY`](#DRY-)
* [`ajax`](#ajax-)
* [`mapping` в `Doctrine`](#mapping-в-Doctrine-)


# Повышение квалификации
* [С какими `CMS`, фреймворками приходилось работать](#С-какими-CMS,-фреймворками-приходилось-работать-)
* [Что самое интересное приходилось делать](#Что-самое-интересное-приходилось-делать-)
* [Что больше всего нравится/не нравится в работе](#Что-больше-всего-нравится/не-нравится-в-работе-)
* [Отношение к работе с чужим кодом](#Отношение-к-работе-с-чужим-кодом-)
* [Интересные проекты / задачи](#Интересные-проекты-/-задачи-)
* [Какой твой любимый язык или фреймворк? Теперь расскажи его минусы.](#Какой-твой-любимый-язык-или-фреймворк?-Теперь-расскажи-его-минусы.-)
* [Почему вообще программируешь и что тебя драйвит?](#Почему-вообще-программируешь-и-что-тебя-драйвит?-)
* [Как получаешь новую информацию](#Как-получаешь-новую-информацию-)
* [какие ресурсы читаются и как часто](#какие-ресурсы-читаются-и-как-часто-)
* [какие задачи интересуют](#какие-задачи-интересуют-)
* [что интересно по жизни, какие хобби](#что-интересно-по-жизни,-какие-хобби-)
* [есть свой блог](#есть-свой-блог-)
* [какие три последние книги прочитал](#какие-три-последние-книги-прочитал-)
* [что сделал в своей жизни такого, чем можешь гордиться](#что-сделал-в-своей-жизни-такого,-чем-можешь-гордиться-)

# Организация разработки

### на каких ОС работал [&uarr;](#devmap)

Ubuntu, mac os, и WSL немного

### базовые знания `UNIX` [&uarr;](#devmap)

#### bash

    Оболочка, или шелл (shell) — это программа, в нашем случае названная «bash», что является сокращением от Bourne Again Shell. 
    Оболочка принимает ваши команды и передаёт их операционной системе. 
    Для взаимодействия с системой используются терминалы, такие как gnome-terminal, eterm, nxterm и т. п.

    Bash - это интерпретатор команд. По сути, это обычная программа, которая запускается при старте сеанса оболочки. 
    Мы могли бы запускать не Bash, а скажем, интерпретатор python или ruby, и тогда нам пришлось бы выполнять методы этих языков вместо команд Bash для администрирования системы.

    Bash принимает команды от пользователя и передает их системному загрузчику, а также обеспечивает взаимодействие между командами, 
    обмен информацией и потоками ввода-вывода. Также оболочка предоставляет пользователю удобный интерфейс для работы с историей команд, поиска и замены, а также исправления ранее выполненных команд, а также автодополнение путей.

#### grep

    Утилита grep решает множество задач, в основном она используется для поиска строк, соответствующих строке в тексте или содержимому файлов. 
    Также она может находить по шаблону или регулярным выражениям. Команда в считанные секунды найдёт файл  с нужной строчкой, текст в файле или отфильтрует из вывода только пару нужных строк. А теперь давайте рассмотрим, как ей пользоваться.
     
    Синтаксис команды выглядит следующим образом:
    
    $ grep [опции] шаблон [имя файла...]
    
    Или:
    
    $ команда | grep [опции] шаблон

    Опции - это дополнительные параметры, с помощью которых указываются различные настройки поиска и вывода, например количество строк или режим инверсии.
    Шаблон - это любая строка или регулярное выражение, по которому будет вестись поиск
    Файл и команда - это то место, где будет вестись поиск. Как вы увидите дальше, grep позволяет искать в нескольких файлах и даже в каталоге, используя рекурсивный режим.


#### alias

    Используйте alias, чтобы поименовать часто используемые команды. Например, alias ll='ls -latr' создаст новое сокращение ll

#### xargs

    Не забывайте использовать xargs (или parallel). Это очень мощная штука. 
    Обратите внимание, что вы можете контролировать количество команд на каждую строку (-L), а также параллельность (-P). Если вы не уверены, что делаете что-то правильно, начните с xargs echo. Еще -I{} – полезная штука. Примеры:

    Возможность объединения нескольких команд Linux в терминале и использования их в качестве конвейера, когда каждая следующая команда получает вывод предыдущей - очень мощный и гибкий инструмент. Но команды можно объединять не только так. 
    С помощью утилиты xargs вывод предыдущей команды можно передать в аргументы следующей.

    Синтаксис команды немного запутанный, но в нём можно разобраться:
    
    $ первая_команда | xargs опции вторая_команда аргументы
    
    Сначала выполняется любая первая команда и весь её вывод по туннелю передается в xargs. Затем этот вывод разбивается на строки и для каждой строки вызывается вторая команда, а полученная строка передаётся ей в аргументах.

      find . -name '*.py' | xargs grep some_function
      cat hosts | xargs -I{} ssh root@{} hostname

#### cd
    Перейти в домашнюю директорию можно с помощью cd. Для указания пути к файлам из домашней директории можно воспользоваться префиксом ~ (например, ~/.bashrc). В sh скриптах для обращения к домашней директории можно использовать переменную $HOME.
    Для того, чтобы перейти к предыдущей рабочей директории, используйте cd -

#### netstat

    Узнайте, какие процессы слушают порты через netstat -lntp или ss -plat (для TCP; добавьте -u для UDP).

#### history

    `Для просмотра последних команд используйте history. Повторить команду: !n (где n - порядковый номер истории). `

#### ssh

    Синтаксис команды выглядит следующим образом:

    $ ssh [опции] имя пользователя@сервер [команда]
    Чтобы просто подключиться к серверу по SSH:
    
    ssh user@host

    Мы привыкли подключаться к удаленному серверу, а уже потом выполнять нужные команды, 
    но на самом деле утилита ssh позволяет сразу выполнить нужную команду без открытия терминала удаленной машины. Например:
    
    ssh user@host ls

### Как найти файл с определенной подстрокой в директории со вложенностями? [&uarr;](#devmap)

    использовать find либо же grep

### Знакомы ли c `Source code management (SCM)` системами - `Git`, `Mercurial`, `SVN`, etc? [&uarr;](#devmap)

Наиболее популярны - Git, с большим отрывом идет SVN (Subversion) и Mercurial.
Долю на рынке, сопоставимую с массой электрона, занимают осталные системы : CVS (Concurrent Versions System), Team Foundation Server, Bazaar, Darcs итд.

Git и Mercurial — распределенные системы, SVN — централизованная.

В первом случае у каждого разработчика на локальной машине хранится полная копия репозитория, и он работает с ней автономно, периодически заливая обновления на сервер с главным репозиторием.

При этом Git, в отличие от Mercurial, работает не только с локальными коммитами, но и с локальными ветками, которые можно вовсе не заливать в удаленный репозиторий.

SVN предполагает, что полная версия кода со всеми ветками хранится в удаленном репозитории, а у разработчика локально находится только тот файл, который он сейчас модифицирует.

C этой точки зрения, SVN лучше вписывается в модель коммерческой разработки, где мы ежедневно контролируем объем и качество выполненной работы.

При использовании Git или Mercurial проектному менеджеру необходимо установить правило ежедневных коммитов для разработчиков, это особенно критично для удаленных команд или аутсорсеров.

### зачем нужны Source code management (SCM) [&uarr;](#devmap)

    Доступ к коду. Исходники кода хранятся в удаленном репозитории (хранилище данных), куда обращаются разработчики, чтобы забрать актуальную версию файлов или внести изменения. Так выстраивается командная разработка.
    
    Логирование изменений в коде. Отслеживание коммитов (внесений изменений в код), помогает найти кто, что и когда менял, решить конфликты при модифицировании одних и тех же файлов, откатиться на любое предыдущее состояние.
    
    Ветвление разработки. Программисты параллельно ведут разработку нового функционала в отдельных ветках, не затрагивая работоспособности старого.
    
    Поддержка версионности продуктов. При выпуске обновлений программных продуктов, мы обозначаем релизные версии, например, с помощью тегов, чтобы зафиксировать их в этом состоянии, для дебага или ретроспективы.

### какими пользовались [&uarr;](#devmap)

Git (Github, Gitlab, Bitbucket немного)

В целом, Bitbucket больше подходит для работы распределенных команд, а GitHub хорош для индивидуальных проектов.

Преимуществами Bitbucket в том, что в нем встроена интеграция с другими продуктами Atlassian. 
Если команда большая удобнее работать с Jira, если небольшая и задач немного, то подойдет и Trello.

### `gitflow`, `cherypick`, `revert` [&uarr;](#devmap)

Git-flow

    Git-flow — альтернативная модель ветвления Git, в которой используются функциональные ветки и несколько основных веток. Эта модель была впервые опубликована и популяризована Винсентом Дриссеном на сайте nvie. По сравнению с моделью магистральной разработки, в Git-flow используется больше веток, каждая из которых существует дольше, а коммиты обычно крупнее. В соответствии с этой моделью разработчики создают функциональную ветку и откладывают ее слияние с главной магистральной веткой до завершения работы над функцией. Такие долгосрочные функциональные ветки требуют тесного взаимодействия разработчиков при слиянии и создают повышенный риск отклонения от магистральной ветки. В них также могут присутствовать конфликтующие обновления.

    Git-flow можно использовать для проектов, в которых запланирован цикл релизов и реализуется характерная для DevOps методика непрерывной поставки. 
    В этом рабочем процессе используются понятия и команды, которые были предложены в рамках рабочего процесса с функциональными ветками. 
    Однако Git-flow привносит новые специфические роли для разных веток и определяет характер и частоту взаимодействия между ними. 
    Помимо функциональных веток в рамках этого рабочего процесса используются отдельные ветки для подготовки, поддержки и регистрации релизов. 
    При этом вы по-прежнему можете пользоваться преимуществами рабочего процесса с функциональными ветками, такими как запросы pull, изолированные эксперименты и эффективное командное взаимодействие.

git cherry-pick

    Команда git cherry-pick берёт изменения, вносимые одним коммитом, и пытается повторно применить их в виде нового коммита в текущей ветке. 
    Эта возможность полезна в ситуации, когда нужно забрать парочку коммитов из другой ветки, а не сливать ветку целиком со всеми внесенными в нее изменениями.

git revert

    Команда git revert — полная противоположность git cherry-pick. 
    Она создаёт новый коммит, который вносит изменения, противоположные указанному коммиту, по существу отменяя его.


### как перенести изменения из одной ветку в другую (2 способа) [&uarr;](#devmap)

merge или rebase

### зачем нужна команда `git rebase` [&uarr;](#devmap)

git rebase — это «автоматизированный» cherry-pick. Он выполняет ту же работу, но для цепочки коммитов, тем самым как бы перенося ветку на новое место.

это наложение коммитов поверх другого базового коммита. Под базовым понимается тот коммит, к которому применяются коммиты выбранной ветки.

Перебазирование в git используется для придания линейности истории ветки, чтобы удобно отслеживать изменения, или для обновления ветки разработки последними изменениями из основной ветки.

### Чем отличается `rebase` от `merge` - описание `rebase`, `merge`, `stash` [&uarr;](#devmap)

Git Merge

Слияние — обычная практика для разработчиков, использующих системы контроля версий. 
Независимо от того, созданы ли ветки для тестирования, исправления ошибок или по другим причинам, слияние фиксирует изменения в другом месте. 
Слияние принимает содержимое ветки источника и объединяет их с целевой веткой. 
В этом процессе изменяется только целевая ветка. История исходных веток остается неизменной.

    Плюсы:

    простота;
    сохраняет полную историю и хронологический порядок;
    поддерживает контекст ветки.


    Минусы:

    история коммитов может быть заполнена (загрязнена) множеством коммитов;
    отладка с использованием git bisect может стать сложнее.

Git Rebase

Rebase — еще один способ перенести изменения из одной ветки в другую. Rebase сжимает все изменения в один «патч». Затем он интегрирует патч в целевую ветку.

В отличие от слияния, перемещение перезаписывает историю, потому что она передает завершенную работу из одной ветки в другую. В процессе устраняется нежелательная история.

    Плюсы:

    Упрощает потенциально сложную историю
    Упрощение манипуляций с единственным коммитом
    Избежание слияния коммитов в занятых репозиториях и ветках
    Очищает промежуточные коммиты, делая их одним коммитом, что полезно для DevOps команд


    Минусы:

    Сжатие фич до нескольких коммитов может скрыть контекст
    Перемещение публичных репозиториев может быть опасным при работе в команде
    Появляется больше работы
    Для восстановления с удаленными ветками требуется принудительный пуш. Это приводит к обновлению всех веток, имеющих одно и то же имя, как локально, так и удаленно, и это ужасно.

    Если вы сделаете перемещение неправильно, история изменится, а это может привести к серьезным проблемам, поэтому убедитесь в том, что делаете!

Git squash

    Git squash — это прием, который помогает взять серию коммитов и уплотнить ее. Например, предположим: у вас есть серия из N коммитов и вы можете путем сжатия преобразовать ее в один-единственный коммит. 
    Сжатие через git squash в основном применяется, чтобы превратить большое число малозначимых коммитов в небольшое число значимых. Так становится легче отслеживать историю Git.

    Также этот прием используется при объединении ветвей. Чаще всего вам будут советовать всегда сжимать коммиты и выполнять перебазирование с родительской ветвью (например, master или develop). 
    В таком случае история главной ветки будет содержать только значимые коммиты, без ненужной детализации.

    сжатие коммитов меняет историю Git, поэтому не рекомендуется сжимать ветвь, если вы уже отправили ее в удаленный репозиторий. 
    Всегда выполняйте сжатие до того, как отправить пуш с изменениями.

    git rebase -i HEAD~3

### разница между `git` и `svn` (если есть) [&uarr;](#devmap)


    SVN – централизованная система контроля версий, Git – децентрализованная
    При ветвлении SVN копирует все содержимое ветки, Git – создает указатели

    Основное различие между этими двумя системами заключается в том, что Git — это распределенная система контроля версий, а Sagainst — централизованная система контроля версий. 
    Это означает, что вы держите репозиторий либо на своей машине (распределенный), так что вы можете работать локально, а затем синхронизировать изменения с общим сервером. В SVN весь код размещается в одном месте, и все разработчики должны быть подключены к нему, чтобы каждый мог синхронизировать и загружать изменения с сервера

### зачем нужна [&uarr;](#devmap)

Задачи системы управления проектами
Поддержка планирования проектов (планирование сроков, ресурсов, финансов). Поддержка принятия решений по реализации проектов компании. Контроль реализации проектов компании. Контроль использования в проектах финансовых, трудовых и материальных ресурсов компании.


### какими пользовались [&uarr;](#devmap)

Redmine - для рабочих проектов
Trello - для личных проектов

### как была организована работа в команде [&uarr;](#devmap)

работа велась по методике Waterfall (каскадная модель)

    Waterfall — методика управления проектами, которая подразумевает последовательный переход с одного этапа на другой без пропусков и возвращений на предыдущие стадии. 

    Agile — система идей и принципов «гибкого» управления проектами, на основе которых разработаны популярные методы Scrum, Kanban и другие. Ключевой принцип — разработка через короткие итерации (циклы), в конце каждого из которых заказчик (пользователь) получает рабочий код или продукт. 

Agile стал основой для целого ряда гибких методик, среди которых наиболее известны Scrum, Lean и экстремальное программирование.

    Scrum — методология гибкой разработки на основе Agile, в основе которого лежит «спринт» — отрезок от 1 до 4 недель, по окончанию которого должна быть получена рабочая версия продукта.

    Lean — метод, который вырос на основе системы управления производством Toyota Production System. В его основе — философия постоянного совершенствования на всех уровнях организации, где одно из ключевых понятий — ценность (то, за что готов платить заказчик).

    Экстремальное программирование (XP) — одна из Agile-методик, где важная роль отводится периодической игре в планирование с привлечением заказчика. Она позволяет определить недостатки предыдущей итерации, приоритетность задач, желаемую функциональность продукта с учётом пожеланий заказчика.

Как я считаю, agile лучше использовать в разработке а waterfall в поддержке.


### методы разрешения конфликтов [&uarr;](#devmap)

    Существует несколько основные проявления в коллективе:

    разногласия между личностью и личностью (межличностный),
    между личностью и группой,
    противостояние групповое (межгрупповой). 

    Основными эмоциональными причинами проявления, проявлениями совместной деятельности, выраженной конфликтогенностью взаимодействия, ощущениями ощущений, проявлениями чувств противоречиями, эмоциональными манипуляциями и проявлениями, выявлением проявлений, сознательным выявлением проявлений, столкновением интересов и пр.


    Основные навыки успешного управления конфликтами

    Определить источник конфликта – взвешенный взгляд людей на одну и ту же проблему, расхождение в частностях, ценностях и назначениях.
    Установить связь конфликтующих сторон – поиск общности целей и желание пойти на взаимные уступки.
    Наладить конструктивный диалог – выявление агрессии, резких эмоций и эмоций, встречать диалог на взаимном уважении. Стремиться к заключению выгодного договора для сторон.
    Создать позитивные связи – чем быстрее наладить коммуникацию, тем быстрее сосредоточиться на спорных вопросах и достижении целей.
### понимание методологии `Scrum` и ее атрибутов (стендапы, грумминг/оценка бэклога, ретроспективы) [&uarr;](#devmap)

В настоящее время, Scrum является одной из наиболее популярных «методологий» разработки ПО. Согласно определению, Scrum — это каркас разработки, с использованием которого люди могут решать появляющиеся проблемы, при этом продуктивно и производя продукты высочайшей значимости

В классическом Scrum существует 3 базовых роли:
-Product owner
-Scrum master
-Команда разработки (Development team)

Product owner (PO) является связующим звеном между командой разработки и заказчиком. Задача PO — максимальное увеличение ценности разрабатываемого продукта и работы команды.

Одним из основных инструментов PO является Product Backlog. Product Backlog содержит необходимые для выполнения рабочие задачи (такие как Story, Bug, Task и др.), отсортированные в порядке приоритета (срочности).

Scrum master (SM) является «служащим лидером» (англ. servant-leader). Задача Scrum Master — помочь команде максимизировать ее эффективность посредством устранения препятствий, помощи, обучении и мотивации команде, помощи PO

Команда разработки (Development team, DT) состоит из специалистов, производящих непосредственную работу над производимым продуктом. 

Рекомендуемый размер команды — 7 (плюс-минус 2) человека. Согласно идеологам Scrum, команды большего размера требуют слишком больших ресурсов на коммуникации, в то время как команды меньшего размера повышают риски (за счет возможного отсутствия требуемых навыков) и уменьшают размер работы, который команда может выполнить в единицу времени.

Процесс Scrum

Основой Scrum является Sprint, в течении которого выполняется работа над продуктом. По окончанию Sprint должна быть получена новая рабочая версия продукта. Sprint всегда ограничен по времени (1-4 недели) и имеет одинаковую продолжительность на протяжении всей жизни продукта.

Перед началом каждого Sprint производится Sprint Planning, на котором производится оценка содержимого Product Backlog и формирование Sprint Backlog, который содержит задачи (Story, Bugs, Tasks), которые должны быть выполнены в текущем спринте. Каждый спринт должен иметь цель, которая является мотивирующим фактором и достигается с помощью выполнения задач из Sprint Backlog.

Каждый день производится Daily Scrum, на котором каждый член команды отвечает на вопросы «что я сделал вчера?», «что я планирую сделать сегодня?», «какие препятствия на своей работе я встретил?». Задача Daily Scrum — определение статуса и прогресса работы над Sprint, раннее обнаружение возникших препятствий, выработка решений по изменению стратегии, необходимых для достижения целей Sprint'а.

По окончанию Sprint'а производятся Sprint Review и Sprint Retrospective, задача которых оценить эффективность (производительность) команды в прошедшем Sprint'е, спрогнозировать ожидаемую эффективность (производительность) в следующем спринте, выявлении имеющихся проблем, оценки вероятности завершения всех необходимых работ по продукту и другое.

# Кэширование

Кэширование (или кэш) – это некий промежуточный буфер, в котором хранятся данные. Благодаря кэшированию страница сайта не воссоздается заново для каждого пользователя. Кэширование позволяет осуществлять работу с большим количеством данных в максимально сжатые сроки и при ограниченных ресурсах (серверных и пользовательских).

### какие виды кэширования использовал [&uarr;](#devmap)

### `Memcache`, `Redis` [&uarr;](#devmap)
### Если использовал `Memcache`, то с какими проблемами сталкивался. [&uarr;](#devmap)

# Тесты

### зачем нужны [&uarr;](#devmap)
### функциональные, стресс, юнит тесты [&uarr;](#devmap)
### что такое `TDD` [&uarr;](#devmap)

# Web специфика

### виды сетевых протоколов [&uarr;](#devmap)

#### IP — Internet Protocol

Протокол передачи, который первым объединил отдельные компьютеры в единую сеть. Самый примитивный в этом списке. Он является ненадёжным, т. е. не подтверждает доставку пакетов получателю и не контролирует целостность данных. По протоколу IP передача данных осуществляется без установки соединения.

Основная задача этого протокола — маршрутизация датаграмм, т. е. определение пути следования данных по узлам сети.

Популярная версия на текущий момент — IPv4 с 32-битными адресами. Это значит, что в интернете могут хранится 4.29 млрд адресов IPv4. Число большое, но не бесконечное. Поэтому существует версия IPv6, которая поможет решить проблему переполнения адресов, ведь уникальных IPv6 будет 2 ^ 128 адресов (число с 38 знаками).

#### TCP/IP — Transmission Control Protocol/Internet Protocol

Это стек протоколов TCP и IP. Первый обеспечивает и контролирует надёжную передачу данных и следит за её целостностью. Второй же отвечает за маршрутизацию для отправки данных. Протокол TCP часто используется более комплексными протоколами.
UDP — User Datagram Protocol

Протокол, обеспечивающий передачу данных без предварительного создания соединения между ними. Этот протокол является ненадёжным. В нём пакеты могут не только не дойти, но и прийти не по порядку или вовсе продублироваться.

Основное преимущество UDP протокола заключается в скорости доставки данных. Именно поэтому чувствительные к сетевым задержкам приложения часто используют этот тип передачи данных.

#### FTP — File Transfer Protocol

Протокол передачи файлов. Его использовали ещё в 1971 году — задолго до появления протокола IP. На текущий момент этим протоколом пользуются при удалённом доступе к хостингам. FTP является надёжным протоколом, поэтому гарантирует передачу данных.

Этот протокол работает по принципу клиент-серверной архитектуры. Пользователь проходит аутентификацию (хотя в отдельных случаях может подключаться анонимно) и получает доступ к файловой системе сервера.
DNS

Это не только система доменных имён (Domain Name System), но и протокол, без которого эта система не смогла бы работать. Он позволяет клиентским компьютерам запрашивать у DNS-сервера IP-адрес какого-либо сайта, а также помогает обмениваться базами данных между серверами DNS. В работе этого протокола также используются TCP и UDP.

#### HTTP — HyperText Transfer Protocol

Изначально протокол передачи HTML-документов. Сейчас же он используется для передачи произвольных данных в интернете. Он является протоколом клиент-серверного взаимодействия без сохранения промежуточного состояния. В роли клиента чаще всего выступает веб-браузер, хотя может быть и, например, поисковый робот. Для обмена информацией протокол HTTP в большинстве случаев использует TCP/IP.

HTTP имеет расширение HTTPS, которое поддерживает шифрование. Данные в нём передаются поверх криптографического протокола TLS.

#### NTP — Network Time Protocol

Не все протоколы передачи нужны для обмена классического вида информацией. NTP — протокол для синхронизации локальных часов устройства со временем в сети. Он использует алгоритм Марзулло. Благодаря нему протокол выбирает более точный источник времени. NTP работает поверх UDP — поэтому ему удаётся достигать большой скорости передачи данных. Протокол достаточно устойчив к изменениям задержек в сети.

Последняя версия NTPv4 способна достигать точности 10мс в интернете и до 0,2мс в локальных сетях.

#### SSH — Secure SHell

Протокол для удалённого управления операционной системой с использованием TCP. В SSH шифруется весь трафик, причём с возможностью выбора алгоритма шифрования. В основном это нужно для передачи паролей и другой важной информации.

Также SSH позволяет обрабатывать любые другие протоколы передачи. Это значит, что кроме удалённого управления компьютером, через протокол можно пропускать любые файлы или даже аудио/видео поток.

SSH часто применяется при работе с хостингами, когда клиент может удалённо подключиться к серверу и работать уже оттуда.

### Что такое `https` и зачем он нужен [&uarr;](#devmap)

HTTPS (от англ. HyperText Transfer Protocol Secure) – расширение протокола HTTP, которое используется для шифрования и безопасного обмена данными между пользователем и сайтом. Вся информация шифруется с помощью криптографических протоколов TSL или его предшественника SSL. Википедия рулит! Если не уходить глубоко в технические подробности, то HTTPS шифрует данные и не дает возможности их перехватить.


Принципом работы защищённого протокола HTTPS является обмен ключами шифрования. Прежде чем ответить на запрос от браузера, сервер предъявляет ключ — SSL-certificate. Браузер проверяет подлинность ключа в Центре сертификации. Если ключ «подошёл», браузер и сервер доверяют друг другу и договариваются о разовом шифре. Так происходит каждую сессию, то есть каждый раз при обмене запросами и ответами. Вот таким хитрым способом и обеспечивается сохранность данных и конфиденциальность при обмене информацией.

Как это работает? Как только Вы зашли на сайт, то Ваш браузер получает от сервера сертификат, в котором есть вся информация о сайте, его владельце, кем выдан и еще много всего. Самое главное, Ваш браузер получает открытый ключ, по которому он начнет шифровать все данные. На сервере стоит закрытый ключ, с помощью которого можно расшифровать всю информацию.

Если кто-то захочет перехватить или получить конфиденциальные данные, то он сможет увидеть только набор с непонятных символов, на расшифровку которых потребуется несколько лет. За это время сертификат устареет, и в итоге злоумышленник ничего не получит.

#### Зачем нужен SSL-сертификат для сайта

    Чтобы сайт стал работать по протоколу безопасного соединения HТТPS, нужен SSL-сертификат. Это виртуальный документ, который содержит данные об организации, её владельце и подтверждает их существование. Позволяет узнать сервер и подтвердить безопасность сайта.

    Использование сертификата безопасности для сайта гарантирует:

        Подлинность ресурса, к которому обращается пользователь. Это повышает у посетителей уровень доверия.
        Целостность передаваемой информации. При транспортировке от сервера к браузеру данные не изменятся и не потеряются.
        Конфиденциальность. 256-разрядное шифрование исключает доступ злоумышленников к информации.

    Что дает SSL-сертификат для сайта кроме защиты данных? SSL-сертификат помогает в SEO-продвижении проекта — позволяет занять более высокую позицию в поисковой выдаче. Поисковые системы (Google, Яндекс и пр.) дорожат доверием аудитории и выше ранжируют сайты, которые работают через безопасное соединение.


### Понимание базовых аспектов функционирования сети - протоколы, `DNS`, и т.д.; [&uarr;](#devmap)

Интернет – это глобальная информационная система, которая:

·        логически взаимосвязана пространством глобальных уникальных адресов, основанных на Интернет-протоколе (IP);

·        способна поддерживать коммуникации с использованием семейства протокола управления передачей - TCP/IP или его последующих расширений/преемников и/или других IP-совместимых протоколов;

·         обеспечивает, использует или делает доступными на общественной или частной основе высокоуровневые услуги, надстроенные над описанной здесь коммуникационной и иной связанной с ней инфраструктурой.

 

Инфраструктура Интернет:

1.магистральный уровень (система связанных высокоскоростных телекоммуникационных серверов).

2.уровень сетей и точек доступа (крупные телекоммуникационные сети), подключенных к магистрали.

3.уровень региональных и других сетей.

4.ISP – интернет-провайдеры.

5.пользователи.


К техническим ресурсам сети Интернет относятся компьютерные узлы, маршрутизаторы, шлюзы, каналы связи и др.

### Что происходит при открытии вкладки браузера [&uarr;](#devmap)


#### 1. Пользователь вводит в браузере адрес сайта
#### 2. Браузер начинает искать сервер

За работу любого сайта обычно отвечает один из миллионов серверов, подключенных к интернету. 
Адрес сервера — это уникальный набор цифр, который называется IP-адресом. 
Например, для vc.ru— это сервер 85.119.149.83.

Поэтому первым делом браузеру нужно понять, какой IP-адрес у сервера, на котором находится сайт.

Такая информация хранится в распределенной системе серверов — DNS (Domain Name System). 
Система работает как общая «контактная книга», хранящаяся на распределенных серверах и устройствах в интернете.

Однако перед тем, как обращаться к DNS, браузер пытается найти запись об IP-адресе сайта в ближайших местах, чтобы сэкономить время:

    Сначала в своей истории подключений. Если пользователь уже посещал сайт, то в браузере могла сохраниться информация c IP-адресом сервера.
    В операционной системе. Не обнаружив информации у себя, браузер обращается к операционной системе, которая также могла сохранить у себя DNS-запись. Например, если подключение с сайтом устанавливалось через одно из установленных на компьютере приложений.
    В кэше роутера, который сохраняет информацию о последних соединениях, совершенных из локальной сети.

#### 3. Браузер отправляет запрос к DNS-серверам

Не обнаружив подходящих записей в кэше, браузер формирует запрос к DNS-серверам, расположенным в интернете.

Например, если нужно найти IP-адрес сайта mail.vc.ru, браузер спрашивает у ближайшего DNS-сервера «Какой IP-адрес у сайта mail.vc.ru?».

Сервер может ответить: «Я не знаю про mail.vc.ru, но знаю сервер, который отвечает за vc.ru». Запрос переадресовывается дальше, на сервер «выше», пока в итоге один из серверов не найдет ответ об IP-адресе для сайта.

#### 4. Браузер устанавливает соединение с сервером

Как только браузер узнал IP-адрес нужного сервера, он пытается установить с ним соединение. В большинстве случаев для этого используется специальный протокол — TCP.

TCP — это набор правил, который описывает способы соединения между устройствами, форматы отправки запросов, действия в случае потери данных и так далее.

Например, для установки соединения между браузером и сервером в стандарте TCP используется система «трёх рукопожатий». Работает она так:

    Устройство пользователя отправляет специальный запрос на установку соединения с сервером — называется SYN-пакет.
    Сервер в ответ отправляет запрос с подтверждением получения SYN-пакета — называется SYN/ACK-пакет.
    В конце устройство пользователя при получении SYN/ACK-пакета отправляет пакет с подтверждением — ACK-пакет. В этот момент соединение считается установленным.

#### 5. Браузер отправляет HTTP-запрос, чтобы получить контент сайта

После установки соединения браузер отправляет специальный запрос, в котором просит сервер отправить данные для отображения страницы. В этом запросе содержится информация о самом браузере, временные файлы, требования к соединению и так далее.

Задача браузера — как можно подробнее объяснить серверу, какая именно информация ему нужна.

В общении браузера и сервера выделяют два типа запросов. GET-запрос используется для получения данных с сервера — например, отобразить картинку, текст или видео. POST-запрос — используется для отправки данных из браузера на сервер, например, когда пользователь отправляет сообщение, картинку или загружает файл.

    Почти все сайты обмениваются информацией с сервером в зашифрованном формате — с помощью HTTPS-протокола. 
    В отличие от HTTP-протокола, в HTTPS используется шифрование, а безопасность подключения подтверждается специальным сертификатом.


#### 6. Сервер обрабатывает запрос

Сервер получил запрос от браузера с подробным описанием того, что ему требуется. 
Теперь ему нужно обработать этот запрос. Этой задачей занимается специальное серверное программное обеспечение — например, nginx или Apache. 
Чаще всего такие программы принято называть веб-серверами.

Веб-сервер в свою очередь перенаправляет запрос на дальнейшую обработку к программе-обработчику — например, PHP, Ruby или ASP.NET. 
Программа внимательно изучает содержимое запроса — например, понимает, в каком формате нужно отправить ответ и какие именно файлы нужны. И собирает ответ.

#### 7. Сервер отправляет ответ браузеру

Когда ответ сформирован, он отправляется веб-сервером обратно браузеру. В ответе как правило содержится контент для отображения веб-страницы, информация о типе сжатия данных, способах кэширования, файлы cookie, которые нужно записать и так далее.
    
    👉 Чтобы обмен данными был быстрым, браузер и сервер обмениваются сразу множеством небольших пакетов данных — как правило, в пределах 8 КБ. 
    Все пакеты имеют специальные номера, которые помогают отслеживать последовательность отправки и получения данных.

#### 8. Браузер обрабатывает полученный ответ и «рисует» веб-страницу

Браузер распаковывает полученный ответ и постепенно начинает отображать полученный контент на экране пользователя — этот процесс называется рендерингом.

Сначала браузер загружает только основную структуру HTML-страницы. Затем последовательно проверяет все теги и отправляет дополнительные GET-запросы для получения с сервера различных элементов — картинки, файлы, скрипты, таблицы стилей и так далее. Поэтому по мере загрузки страницы браузер и сервер продолжают обмениваться между собой информацией.

Параллельно с этим на компьютер как правило сохраняются статичные файлы пользователя — чтобы при следующем посещении не загружать их заново и быстрее отобразить пользователю содержимое страницы.

Как только рендеринг завершен — пользователю отобразится полностью загруженная страница сайта.




### C каким серверным ПО приходилось работать? [&uarr;](#devmap)

Веб-сервер - Apache, Nginx. Веб-сервер — это специальная программа, которая принимает запросы пользователей, обрабатывает их и отправляет ответ обратно по протоколу прикладного уровня HTTP.

Интерпретатор языка программирования

СУБД — система управления базами данных: MySQL, PostgreSQL, MS SQL, Oracle, Redis, MongoDB и т.д.

Поисковые системы — ElasticSearch / Sphinx — позволяют осуществлять поиск и фильтрацию быстрее, нежели это возможно с использованием реляционных СУБД. Kibana - для визуализации данных, полученных из Elasticsearch - это в рамках настройки хранилища для логов на базе Elasticsearch, Logstash и Kibana, которое называют ELK Stack. Sphinx - движок полнотекстового поиска.

Elasticsearch (далее ES) — масштабируемая поисковая система, которую также можно отнести к нереляционным (noSQL) базам данных. В основном используется для полнотекстового поиска с фильтрами и анализаторами.


Кеширующие сервера — системы, «запоминающие» результат обработки запросов и использующие эти данные при повторных обращениях для ускорения генерации страниц — Memcached и Redis.

Софт для резервного копирования — бэкапы должны создаваться регулярно и автоматически, а также хранится не на том же сервере, где расположены «боевые данные».

Ускорители исполнения программного кода. Служат для повышения производительности, часто используемые ускорители для PHP: APC, eAccellerator, XCache.

Мониторинг и оповещения — системы, собирающие важные метрики производительности системы и сообщающие о проблемах.

### Что такое `Apache` и `mod_rewrite`? [&uarr;](#devmap)

Apache – это свободное программное обеспечение для размещения веб-сервера. Он хорошо показывает себя в работе с масштабными проектами.

mod_rewrite — это модуль для веб-сервера Apache, предназначенный для преобразования URL-ов. Модуль использует в своей работе правила, которые могут быть описаны как в конфигурации сервера (httpd.conf), так и в файлах .htaccess непосредственно в файловой структуре Вашего сайта. Правила описываются в виде регулярных выражений 

### `nginx`, его отличие от `apache` [&uarr;](#devmap)

#### Краткий обзор Apache

Apache был разработан для доставки веб-контента, доступ к которому осуществляется через Интернет. Он известен тем, что играл ключевую роль в начальном росте интернета. Apache - это программное обеспечение с открытым исходным кодом, разработанное и поддерживаемое открытым сообществом разработчиков и работающее в самых разных операционных системах. Архитектура включает в себя ядро Apache и модули. Основной компонент предоставляет базовую серверную функцию, поэтому он принимает соединения и управляет параллелизмом. Различные модули соответствуют различным функциям, которые выполняются по каждому запросу. Конкретное развертывание Apache может быть сконфигурировано для включения различных модулей, таких как функции безопасности, управление динамическим контентом или для базовой обработки HTTP-запросов.

Модель «один сервер делает все» стала ключом к раннему успеху Apache. Однако по мере увеличения уровня трафика и увеличения количества веб-страниц и ограничения производительности настройка Apache на работу с реальным трафиком усложнялась.

#### Краткий обзор Nginx

Nginx был разработан специально для устранения ограничений производительности веб-серверов Apache. Производительность и масштабируемость Nginx обусловлены архитектурой, управляемой событиями. Он значительно отличается от подхода Apache. В Nginx каждый рабочий процесс может одновременно обрабатывать тысячи HTTP-соединений. Следовательно, Nginx - это легковесная, масштабируемая и высокопроизводительная реализация. Эта архитектура делает обработку больших и флуктуирующих нагрузок на данные гораздо более предсказуемой с точки зрения использования ОЗУ, использования ЦП и задержки.

Nginx также имеет богатый набор функций и может выполнять различные роли сервера:

    Обратный прокси-сервер для протоколов HTTP, HTTPS, SMTP, POP3 и IMAP
    Балансировщик нагрузки и HTTP-кеш
    Интерфейсный прокси для Apache и других веб-серверов, сочетающий гибкость Apache с хорошей производительностью статического контента Nginx

#### Apache против Nginx: сравнение их богатых наборов функций
#### Простота

Разрабатывать и обновлять приложения на Apache очень просто. Модель «одно соединение на процесс» позволяет очень легко вставлять модули в любой точке логики веб-обслуживания. Разработчики могут добавлять код таким образом, что в случае сбоев будет затронут только рабочий процесс, выполняющий код. Обработка всех других соединений будет продолжаться без помех.

Nginx, с другой стороны, имеет сложную архитектуру, поэтому разработка модулей не легка. Разработчики модулей Nginx должны быть очень осторожны, чтобы создавать эффективный и точный код, без сбоев, и соответствующим образом взаимодействовать со сложным ядром, управляемым событиями, чтобы избежать блокирования операций.

#### Производительность

Производительность измеряется тем, как сервер доставляет большие объемы контента в браузер клиента, и это важный фактор. Контент может быть статическим или динамическим. Давайте посмотрим статистику по этому вопросу.
Статический контент

Nginx работает в 2,5 раза быстрее, чем Apache, согласно тесту производительности, выполняемому до 1000 одновременных подключений. Другой тест с 512 одновременными подключениями показал, что Nginx примерно в два раза быстрее и потребляет меньше памяти. Несомненно, Nginx имеет преимущество перед Apache со статическим контентом. Поэтому, если вам нужно обслуживать одновременный статический контент, Nginx является предпочтительным выбором.

#### Динамический контент

Результаты тестов Speedemy показали, что для динамического контента производительность серверов Apache и Nginx была одинаковой. Вероятная причина этого заключается в том, что почти все время обработки запросов расходуется в среде выполнения PHP, а не в основной части веб-сервера. Среда выполнения PHP довольно похожа для обоих веб-серверов.

Apache также может обрабатывать динамический контент, встраивая процессор языка, подобного PHP, в каждый из его рабочих экземпляров. Это позволяет ему выполнять динамический контент на самом веб-сервере, не полагаясь на внешние компоненты. Эти динамические процессоры могут быть включены с помощью динамически загружаемых модулей.

Nginx не имеет возможности обрабатывать динамический контент изначально. Чтобы обрабатывать PHP и другие запросы на динамический контент, Nginx должен перейти на внешний процессор для выполнения и дождаться отправки визуализированного контента. Однако этот метод также имеет некоторые преимущества. Поскольку динамический интерпретатор не встроен в рабочий процесс, его издержки будут присутствовать только для динамического содержимого.

#### Поддержка ОС

Apache работает во всех операционных системах, таких как UNIX, Linux или BSD, и полностью поддерживает Microsoft Windows. Nginx также работает на нескольких современных Unix-подобных системах и поддерживает Windows, но его производительность в Windows не так стабильна, как на платформах UNIX.
Безопасность

И Apache, и Nginx являются безопасными веб-серверами. Apache Security Team существует, чтобы предоставить помощь и советы проектам Apache по вопросам безопасности и координировать обработку уязвимостей безопасности. Важно правильно настроить серверы и знать, что делает каждый параметр в настройках.

#### Гибкость

Веб-серверы могут быть настроены путем добавления модулей. Apache долго загружал динамические модули, поэтому все модули Apache поддерживают это.

Nginx Plus (Nginx Plus - это программный балансировщик нагрузки, веб-сервер и кэш контента, построенный на основе открытого исходного кода Nginx) также использует модульную архитектуру. Новые функции и возможности могут быть добавлены с программными модулями, которые могут быть подключены к работающему экземпляру Nginx Plus по требованию. Динамические модули добавляют в Nginx Plus такие функции, как геолокация пользователей по IP-адресу, изменение размеров изображений и встраивание сценариев Lua в модель обработки событий Nginx Plus. Модули создаются как Nginx, Inc., так и сторонними разработчиками.

Большинство необходимых функциональных возможностей основного модуля (например, прокси, кэширование, распределение нагрузки) поддерживаются обоими веб-серверами.

#### Поддержка и документация

Важным моментом, который следует учитывать, является доступная справка и поддержка веб-серверов среди прочего программного обеспечения. Поскольку Apache был популярен так долго, поддержка сервера довольно распространена повсеместно. Для главного сервера и для основанных на задачах сценариев, связанных с подключением Apache к другому программному обеспечению, имеется большая библиотека документации первого и стороннего производителя.

Наряду с документацией многие инструменты и веб-проекты содержат инструменты для начальной загрузки в среде Apache. Это может быть включено в сами проекты или в пакеты, поддерживаемые отделом упаковки вашего дистрибутива.

Apache, как правило, получает большую поддержку от сторонних проектов просто из-за своей доли рынка и продолжительности времени, в течение которого он был доступен.

В прошлом для Nginx было трудно найти исчерпывающую англоязычную документацию из-за того, что большая часть ранней разработки и документации была на русском языке. Однако на сегодняшний день документация заполнена, и на сайте Nginx имеется множество ресурсов для администрирования и доступной документации от третьих лиц. 

#### Nginx и Apache - Совместная работа

Для многих приложений Nginx и Apache хорошо дополняют друг друга. Очень распространенным начальным шаблоном является развертывание программного обеспечения Nginx с открытым исходным кодом в качестве прокси-сервера (или Nginx Plus в качестве платформы доставки приложений) перед веб-приложением на основе Apache. Nginx выполняет тяжелую работу, связанную с HTTP - обслуживает статические файлы, кэширует содержимое и разряжает медленные HTTP-соединения - так что сервер Apache может выполнять код приложения в безопасной и защищенной среде. 


### балансировка нагрузки на сервера приложений (`haproxy`) [&uarr;](#devmap)

HAProxy, или High Availability Proxy, является программным балансировщиком нагрузки TCP/HTTP. Он распределяет рабочую нагрузку по серверам для обеспечения максимальной производительности и оптимизации использования ресурсов. HAProxy поддерживает гибко настраиваемые методы проверки доступности, обработки отказов и восстановления после них.

HAProxy устанавливается на отдельный сервер, который принимает клиентские запросы и перенаправлять их на веб-сервера Nginx. 

### работы с системами очередей (`RabbitMQ`, `Kafka`) [&uarr;](#devmap)

#### Apache Kafka

Apache Kafka – это распределенная платформа потоковой передачи событий с открытым исходным кодом, обеспечивающая высокую пропускную способность. Написанная на Java и Scala, Кафка представляет собой шину сообщений системы Pub/Sub, ориентированную на потоки и воспроизведение данных с высокой интенсивностью. Кафка не полагается на очередь, а добавляет сообщения в журнал и оставляет их там до достижения предела хранения или тех пор, пока консьюмер не прочитает эти сообщения.

Apache Kafka лучше всего подходит для потоковой передачи от А к Б без сложной маршрутизации, но с максимальной пропускной способностью. Инструмент отлично справляется с event sourcing, потоковой обработкой и моделированием изменений в системе в качестве последовательности событий. Кафку также можно использовать для обработки данных при многоэтапной конвейерной обработке.

Кафка станет отличным решением, если вам нужен фреймворк для хранения, чтения, повторного чтения и анализа потоковых данных. Ее сильная сторона – обработка и анализ данных в реальном времени. Инструмент идеально подходит для постоянного хранения сообщений или для регулярно проверяемых систем.

#### RabbitMQ

RabbitMQ – это распределенный брокер сообщений с открытым исходным кодом, который обеспечивает эффективную доставку сообщений в рамках сложных сценариев маршрутизации. Этот инструмент называется «распределенным», потому что обычно работает как кластер узлов, где очереди распределяются (реплицируются) по узлам для обеспечения высокой доступности и отказоустойчивости.

По умолчанию в RabbitMQ используется протокол AMQP 0.9.1, также существуют расширения для поддержки дополнительных протоколов: AMQP 1.0, HTTP, STOMP и MQTT. RabbitMQ официально поддерживает Elixir, Go, Java, JavaScript, .NET, PHP, Python, Ruby, Objective-C, Spring и Swift. Пользователям доступны различные инструменты разработки и клиенты, использующие расширения сообщества.

Разработчики используют RabbitMQ для обработки высокопроизводительных и надежных фоновых заданий, а также для интеграции и взаимодействия внутри приложений и между ними. Инструмент применяется для выполнения сложной маршрутизации к консьюмерам и интеграции нескольких приложений и служб с нетривиальной логикой маршрутизации.

RabbitMQ идеально подходит для веб-серверов, которым требуется быстрый запрос-ответ. Этот инструмент распределяет нагрузку между рабочими приложениями при высокой нагрузке (более 20 000 сообщений в секунду) и может обрабатывать фоновые задания или длительные задачи, такие как преобразование PDF, сканирование файлов или масштабирование изображений.

#### Основные отличия Apache Kafka и RabbitMQ

    Поток данных. RabbitMQ использует определенный ограниченный поток данных. Продюсер создает и отправляет сообщения, а консьюмер их принимает. Apache Kafka использует неограниченный поток данных, при этом пары «ключ-значение» непрерывно передаются в назначенную тему.

    Использование данных. RabbitMQ отлично подходит для запросов пользователей и транзакционных данных, таких как создание и размещение заказов. Кафка лучше справляется с операционными данными, такими как технологические процессы, статистика аудита и сбора данных, активность системы.

    Обмен сообщениями. RabbitMQ отправляет пользователям сообщения, которые удаляются из очереди после их обработки и подтверждения. Кафка – это журнал. Он использует непрерывные цепочки сообщений, которые сохраняются в очереди до истечения срока хранения.

    Модель проектирования. RabbitMQ использует модель умный брокер/тупой консьюмер. Брокер последовательно доставляет сообщения консьюмерам и отслеживает их статус. Apache Kafka использует модель тупого брокера/умного консьюмера. Этот инструмент не отслеживает сообщения, которые прочитал каждый пользователь. Кафка запоминает только непрочитанные сообщения, сохраняя их в течение установленного периода времени. Консьюмеры должны самостоятельно следить за своей позицией в каждом журнале.

    Топология. RabbitMQ использует топологию обмена очереди: сообщения отправляются на обмен, откуда затем рассылаются в различные привязки очередей для использования консьюмерами. Кафка использует топологию Publish/Subscribe, отправляя сообщения через поток в соответствующие топики, которые затем потребляются пользователями в разных авторизованных группах.

### `CI` (`Continuous Integration`) [&uarr;](#devmap)

CI/CD означает «Continuous Integration/Continuous Delivery» — то есть «непрерывная интеграция/непрерывная доставка». Это подход к разработке, при котором задачи сборки, публикации, тестирования продукта полностью или частично автоматизированы. Очень часто автоматизация интегрирована в бизнес-процессы продуктовой команды или компании, но практики CI/CD прекрасно могут быть внедрены и в проекты, в которых участвует только один разработчик.

Существует большое количество инструментов для сборки кода и публикации его для пользователей. У каждого свои особенности и тонкости использования. Но есть и конкурирующие между собой инструменты. Среди конкурирующих платформ стоит говорить о GitLab CI/CD, Bitbucket Pipelines, Jenkins, Netlify, JetBrains TeamCity, GitHub Actions и прочие.

Здесь я кратко и по возможности без огрехов опишу процесс работы системы с высоты птичьего полёта:

    разработчик отпраляет коммит в репозиторий, создаёт merge request через сайт, или ещё каким-либо образом явно или неявно запускает пайплайн,
    из конфигурации выбираются все задачи, условия которых позволяют их запустить в данном контексте,
    задачи организуются в соответствии со своими этапами,
    этапы по очереди выполняются — т.е. параллельно выполняются все задачи этого этапа,
    если этап завершается неудачей (т.е. завершается неудачей хотя бы одна из задач этапа) — пайплайн останавливается (почти всегда),
    если все этапы завершены успешно, пайплайн считается успешно прошедшим.


Таким образом, имеем:

    пайплайн — набор задач, организованных в этапы, в котором можно собрать, протестировать, упаковать код, развернуть готовую сборку в облачный сервис, и пр.,
    этап (stage) — единица организации пайплайна, содержит 1+ задачу,
    задача (job) — единица работы в пайплайне. Состоит из скрипта (обязательно), условий запуска, настроек публикации/кеширования артефактов и много другого.


Соответственно, задача при настройке CI/CD сводится к тому, чтобы создать набор задач, реализующих все необходимые действия для сборки, тестирования и публикации кода и артефактов.

### Деплой [&uarr;](#devmap)



### `Composer` [&uarr;](#devmap)



### `docker` [&uarr;](#devmap)

Docker — это технология, которая позволяет создавать и использовать приложения в «родном» окружении. В основе Docker лежит идея: если приложение работает у вас, то оно должно работать где угодно. Способ этого добиться очень простой — нужно упаковать настройки окружения вместе с приложением.

Docker чаще всего применяется для развёртывания серверных приложений, но может использоваться и в мире фронтенда.

Docker Compose — это инструмент для запуска мультиконтейнерного приложения, которое не зависит от платформы и содержит все необходимые для работы технологии и библиотеки. Конфигурация такого приложения записывается в одном текстовом файле в формате YAML. Запускается приложение одной командой в терминале.

В файле compose.yaml могут быть следующие элементы верхнего уровня:

— version (скоро исключат): информация о версии формата файла конфигурации;
— services (обязательный): список всех контейнеров, которые нужно будет запустить;
— networks: список подсетей Docker Network, которые объединяют группы контейнеров в виртуальную локальную сеть (она может быть доступна из внешнего мира);
— volumes: список томов, которые будут доступны контейнерам, описанным в файле конфигурации;
— configs: список параметров, которые позволяют запускать контейнеры в различных режимах, не собирая их заново;
— secrets: список чувствительных с точки зрения безопасности параметров (то же, что и configs, но специального назначения).

Обычно жизненный цикл контейнера состоит из следующей последовательности состояний:

    Создание контейнера
    Работа контейнера
    Приостановка контейнера
    Возобновление работы контейнера
    Запуск контейнера
    Остановка контейнера
    Перезапуск контейнера
    Принудительная остановка контейнера
    Удаление контейнера


### Важные службы

Движок Docker Engine — приложение для управления объектами Docker. Оно включает в себя три компонента:

    сервер (Docker Daemon);
    интерфейс (Docker API);
    консольный клиент (Docker CLI).

Ваш компьютер называется Docker Host. Все операции, которые мы выполняем в интерфейсе или через консоль, выполняются сервером через API движка.

Docker Desktop — пакет приложений с графическим интерфейсом, включающий специальную виртуальную машину для работы с движком, визуальный интерфейс (Dashboard), консольный клиент, инструменты для работы с реестром Docker Hub и пр.

### Объекты Docker

    Образ (Docker Image) — прототип будущего контейнера, содержащий операционную систему, приложение или проект для сборки приложения. Образы состоят из слоёв. Каждый новый слой — это надстройка над предыдущим. Слои должны надстраиваться поверх базового образа, формируя новый. Например, базовым образом может быть образ операционной системы.

    Контейнер (Docker Container) — уже собранное и запущенное приложение в изолированном окружении, которое формируется послойно, в соответствии с образом. Каждый новый слой расширяет функциональность предыдущего, формируя стек используемых инструментов, платформ и настроек системных служб. Файловая система контейнера тоже стековая (Union File Systems). Каталоги и файлы отдельного слоя образа накладываются друг на друга, образуя единое целое.

    Том (Docker Volume) — папка, которую можно подключить (говорят «примонтировать») к контейнерам. Папка может быть связана с конкретной папкой на вашем компьютере, а может быть как бы сетевой для контейнеров на вашем компьютере. Тома необходимы для хранения файлов конфигурации, критических с точки зрения безопасности, файлов баз данных, файлов, которые нельзя удалять после окончания работы приложения.

    Сеть (Docker Network) — виртуальная локальная сеть, которая позволяет совместно использовать несколько запущенных контейнеров и соединять запущенный контейнер с вашим компьютером. В основном вы будете использовать три режима работы сетевой инфраструктуры Docker:

    bridge — когда контейнеры могут взаимодействовать между собой как веб-сервер и база данных.
    host — для доступа к локальному сетевому окружению на вашем компьютере.
    none — сеть для контейнеров полностью отключена.

### Инструменты

Docker Hub (реестр) — официальный реестр образов.

Опубликованные образы хранятся в Docker Hub. Существуют и другие публичные реестры образов:

    Google Cloud Container Registry;
    Azure Container Registry;
    IBM Cloud Container Registry;
    Oracle Cloud Infrastructure Container Registry;
    Yandex Container Registry.

Docker CLI — консольный клиент, позволяющий управлять Docker через интерфейс командной строки.

Консольный клиент содержит команды для управления объектами Docker. Список основных команд:

    docker ps;
    docker run;
    docker image;
    docker container;
    docker volume.


### Как пользоваться

Ключи командного интерфейса Docker CLI хорошо проработаны и похожи на консольные команды в bash. Например, дополнительный ключ prune позволяет удалять неиспользуемые объекты. Ключ rm служит для удаления, а ключ ls для просмотра объектов. Объекты Docker в обязательном порядке имеют уникальное имя. Если вы не именуете объект специально, то имя объекта формируется с помощью хэш-функции. Если вы попытаетесь создать объект одного и того же типа с уже использованным именем, в этом вам будет отказано.

Мониторинг запущенных контейнеров

    docker ps — просмотр запущенных контейнеров.
    docker ps -a — ключ -a выводит и запущенные, и остановленные контейнеры.
    docker ps -s — ключ -s выводит дисковое пространство, используемое каждым запущенным контейнером.
    docker ps -f name=hello — ключ -f фильтрует список контейнеров по имени, например, hello.

### Запуск контейнеров

Для запуска контейнера, который доступен локально или на Docker Hub, выполните команду:

    docker run --name test -i -t hello 
    или
    docker exec -it тут_имя_контейнера bash

Ключ --name используется для установки имени запущенного контейнера. Ключи -i и -t указывают, что для запуска контейнера будет использоваться стандартный поток ввода и терминал TTY соответственно. Для того чтобы при запуске контейнера примонтировать том, который будет связан с папкой на вашем компьютере, а потом получить доступ к контейнеру через терминал, выполните команду:

    docker run -t -i --mount type=bind,src=/data,dst=/data hello bash

### Управление образами

Вы можете получить список всех доступных локально образов с помощью команды:

    docker image ls

Ключи prune, rm действуют обычным способом, позволяя удалить неиспользуемые или конкретные образы соответственно. Для работы с реестром необходимо использовать следующие команды:

    docker image pull hello — загрузка образа с именем hello из реестра;
    docker image push hello — отправка образа с именем hello в реестр;
    docker image inspect hello — полная информация о контейнере hello;
    docker image build — собрать контейнер из текущей папки с учётом Dockerfile.


### Периодическая чистка данных Докера

    docker container prune
    docker image prune
    docker volume prune
    docker network prune

### скачать все образы докера

    docker pull docker.ru/php/php-apache -a
    docker pull docker.ru/php/php-fpm -a

### Управление томами

    docker volume ls — вывод всех томов.
    docker volume ls -f name=hello — вывод всех томов с фильтрацией по имени, например, hello.
    docker volume create hello — создание нового тома, например, hello.
    docker volume inspect hello — исчерпывающая информация о томе.


### Стандарты написания кода [&uarr;](#devmap)
### Шаги по оптимизации сайта [&uarr;](#devmap)

# Разработка

### Интересный крон (запуск скрипта раз в 30 секунд) [&uarr;](#devmap)

Крон это минимум минута.
Раз в 30 секунд это значит запускать каждую минуту скрипт который что-то делает, потом спит 30 секунд и еще раз делает.
Как вариант, написать скрипт, который будет запускаться раз в минуту и дёргать файл дважды, между двумя дёрганьями спать 30 секунд.

### Защита от спама - предложить интересный способ [&uarr;](#devmap)

Google ReCaptcha
Блэк лист
Hidden поле которое потом сможем проверять на бэке

### Защита от повторной отправки форм [&uarr;](#devmap)

Ограниченои частоты отправки сообщений
Ограничение количества сообщний от одного пользователя
ограничение количества отправляемых сообщений за определенный отрезок времени

### Сортировка пузырьком, сложность алгоритма [&uarr;](#devmap)
### Веб-сервисы, отличия, когда что использовать (`soap`, архитектура `rest`) [&uarr;](#devmap)
### `raise condition` [&uarr;](#devmap)
### `CORS` [&uarr;](#devmap)
### `SPINX` [&uarr;](#devmap)
### гексагональная архитектура [&uarr;](#devmap)
### `reflection API` [&uarr;](#devmap)
### `SPL` [&uarr;](#devmap)
### `SOLID` [&uarr;](#devmap)
### `DRY` [&uarr;](#devmap)
### `ajax` [&uarr;](#devmap)
### `mapping` в `Doctrine` [&uarr;](#devmap)

# Повышение квалификации

### С какими `CMS`, фреймворками приходилось работать [&uarr;](#devmap)
### Что самое интересное приходилось делать [&uarr;](#devmap)
### Что больше всего нравится/не нравится в работе [&uarr;](#devmap)
### Отношение к работе с чужим кодом [&uarr;](#devmap)
### Интересные проекты / задачи [&uarr;](#devmap)
### Какой твой любимый язык или фреймворк? Теперь расскажи его минусы. [&uarr;](#devmap)
### Почему вообще программируешь и что тебя драйвит? [&uarr;](#devmap)
### Как получаешь новую информацию [&uarr;](#devmap)
### какие ресурсы читаются и как часто [&uarr;](#devmap)
### какие задачи интересуют [&uarr;](#devmap)
### что интересно по жизни, какие хобби [&uarr;](#devmap)
### есть свой блог [&uarr;](#devmap)
### какие три последние книги прочитал [&uarr;](#devmap)
### что сделал в своей жизни такого, чем можешь гордиться [&uarr;](#devmap)
