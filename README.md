# devmap

# Организация разработки
   ### Система
   * [на каких ОС работал](#на-каких-ОС-работал-)
   * [базовые знания `UNIX`](#базовые-знания-UNIX-)
   * Основы работы с файлами
   * [Как найти файл с определенной подстрокой в директории со вложенностями?](#Как-найти-файл-с-определенной-подстрокой-в-директории-со-вложенностями?-)
   ### Система контроля версий
   * [Знакомы ли c `Source code management (SCM)` системами - `Git`, `Mercurial`, `SVN`, etc?](#Знакомы-ли-c-SCM-системами---Git,-Mercurial,-SVN,-etc?-)
   * [зачем нужны `Source code management (SCM)`](#зачем-нужна-)
   * [какими пользовались](#какими-пользовались-)
   ### Git
   * [`gitflow`, `cherypick`, `revert`](#gitflow,-cherypick-)
   * [как перенести изменения из одной ветку в другую (2 способа)](#как-перенести-изменения-из-одной-ветку-в-другую-(2-способа)-)
   * [зачем нужна команда `git rebase`](#зачем-нужна-команда-git-rebase-)
   * [`rebase`, `merge`, `stash`](#Чем-отличается-rebase-от-merge-)
   * [разница между `git` и `svn` (если есть)](#разница-между-git-и-svn-(если-есть)-)
   ### Система тикетов и организация задач
   * [зачем нужна](#зачем-нужна-)
   * [какими пользовались](#какими-пользовались-)
   * [как была организована работа в команде](#как-была-организована-работа-в-команде-)
   * [методы разрешения конфликтов](#методы-разрешения-конфликтов-)
   * [понимание методологии `Scrum` и ее атрибутов (стендапы, грумминг/оценка бэклога, ретроспективы)](#понимание-методологии-Scrum-и-ее-атрибутов-(стендапы,-грумминг/оценка-бэклога,-ретроспективы)-)


# Кэширование!!!
* [какие виды кэширования использовал](#какие-виды-кэширования-использовал-)
* [`Memcache`, `Redis`](#Memcache,-Redis-)
* [Если использовал `Memcache`, то с какими проблемами сталкивался.](#Если-использовал-Memcache,-то-с-какими-проблемами-сталкивался.-)

# Тесты!!!
* [зачем нужны](#зачем-нужны-)
* [функциональные, стресс, юнит тесты](#функциональные,-стресс,-юнит-тесты-)
* [что такое `TDD`](#что-такое-TDD-)


# Web специфика
* [виды сетевых протоколов](#виды-сетевых-протоколов-)
* [Что такое `https` и зачем он нужен](#Что-такое-https-и-зачем-он-нужен-)
* [Понимание базовых аспектов функционирования сети - протоколы, `DNS`, и т.д.;](#Понимание-базовых-аспектов-функционирования-сети---протоколы,-DNS,-и-т.д.;-)
* [Что происходит при открытии вкладки браузера](#Что-происходит-при-открытии-вкладки-браузера-)
* [C каким серверным ПО приходилось работать?](#C-каким-серверным-ПО-приходилось-работать?-)
* [Что такое `Apache` и `mod_rewrite`?](#Что-такое-Apache-и-mod_rewrite?-)
* [`nginx`, его отличие от `apache`](#nginx,-его-отличие-от-apache-)
* [балансировка нагрузки на сервера приложений (`haproxy`)](#балансировка-нагрузки-на-сервера-приложений-(haproxy)-)
* [работы с системами очередей (`RabbitMQ`, `Kafka`)](#работы-с-системами-очередей-(RabbitMQ,-Kafka)-)
* [`CI` (`Continuous Integration`)](#CI-(Continuous-Integration)-)
* [Деплой](#Деплой-)
* [`Composer`](#Composer-)
* [`docker`](#docker-)
* [Стандарты написания кода](#Стандарты-написания-кода-)
* [Шаги по оптимизации сайта](#Шаги-по-оптимизации-сайта-)


# Разработка
* [Интересный крон (запуск скрипта раз в 30 секунд)](#Интересный-крон-(запуск-скрипта-раз-в-30-секунд)-)
* [Защита от спама - предложить интересный способ](#Защита-от-спама---предложить-интересный-способ-)
* [Защита от повторной отправки форм](#Защита-от-повторной-отправки-форм-)
* [Сравнение алгоритмов сортировки](#Сравнение-алгоритмов-сортировки-)
* [Веб-сервисы, отличия, когда что использовать (`soap`, архитектура `rest`)](#Веб-сервисы,-отличия,-когда-что-использовать-(soap,-архитектура-rest)-)
* [`raise condition`](#raise-condition-)
* [`CORS`](#CORS-)
* [`SPINX`](#SPINX-)
* [`Elasticsearch`](#Elasticsearch-)
* [гексагональная архитектура](#гексагональная-архитектура-)
* [`KISS, DRY, YAGNI, BDUF, SOLID, APO и бритва Оккама`](#KISS,-DRY,-YAGNI,-BDUF,-SOLID,-APO-и-бритва-Оккама-)
* [`mapping` в `Doctrine`](#mapping-в-Doctrine-)


# Повышение квалификации
* [С какими `CMS`, фреймворками приходилось работать](#С-какими-CMS,-фреймворками-приходилось-работать-)
* [Что больше всего нравится/не нравится в работе](#Что-больше-всего-нравится/не-нравится-в-работе-)
* [Отношение к работе с чужим кодом](#Отношение-к-работе-с-чужим-кодом-)
* [Интересные проекты / задачи](#Интересные-проекты-/-задачи-)
* [Какой твой любимый язык или фреймворк? Теперь расскажи его минусы.](#Какой-твой-любимый-язык-или-фреймворк?-Теперь-расскажи-его-минусы.-)
* [Почему вообще программируешь и что тебя драйвит?](#Почему-вообще-программируешь-и-что-тебя-драйвит?-)
* [Как получаешь новую информацию](#Как-получаешь-новую-информацию-)
* [какие задачи интересуют](#какие-задачи-интересуют-)
* [что интересно по жизни, какие хобби](#что-интересно-по-жизни,-какие-хобби-)
* [какие три последние книги прочитал](#какие-три-последние-книги-прочитал-)
* [что сделал в своей жизни такого, чем можешь гордиться](#что-сделал-в-своей-жизни-такого,-чем-можешь-гордиться-)

# Организация разработки

### на каких ОС работал [&uarr;](#devmap)

Ubuntu, mac os, и WSL немного

### базовые знания `UNIX` [&uarr;](#devmap)

#### bash

    Оболочка, или шелл (shell) — это программа, в нашем случае названная «bash», что является сокращением от Bourne Again Shell. 
    Оболочка принимает ваши команды и передаёт их операционной системе. 
    Для взаимодействия с системой используются терминалы, такие как gnome-terminal, eterm, nxterm и т. п.

    Bash - это интерпретатор команд. По сути, это обычная программа, которая запускается при старте сеанса оболочки. 
    Мы могли бы запускать не Bash, а скажем, интерпретатор python или ruby, и тогда нам пришлось бы выполнять методы этих языков вместо команд Bash для администрирования системы.

    Bash принимает команды от пользователя и передает их системному загрузчику, а также обеспечивает взаимодействие между командами, 
    обмен информацией и потоками ввода-вывода. Также оболочка предоставляет пользователю удобный интерфейс для работы с историей команд, поиска и замены, а также исправления ранее выполненных команд, а также автодополнение путей.

#### grep

    Утилита grep решает множество задач, в основном она используется для поиска строк, соответствующих строке в тексте или содержимому файлов. 
    Также она может находить по шаблону или регулярным выражениям. Команда в считанные секунды найдёт файл  с нужной строчкой, текст в файле или отфильтрует из вывода только пару нужных строк. А теперь давайте рассмотрим, как ей пользоваться.
     
    Синтаксис команды выглядит следующим образом:
    
    $ grep [опции] шаблон [имя файла...]
    
    Или:
    
    $ команда | grep [опции] шаблон

    Опции - это дополнительные параметры, с помощью которых указываются различные настройки поиска и вывода, например количество строк или режим инверсии.
    Шаблон - это любая строка или регулярное выражение, по которому будет вестись поиск
    Файл и команда - это то место, где будет вестись поиск. Как вы увидите дальше, grep позволяет искать в нескольких файлах и даже в каталоге, используя рекурсивный режим.


#### alias

    Используйте alias, чтобы поименовать часто используемые команды. Например, alias ll='ls -latr' создаст новое сокращение ll

#### xargs

    Не забывайте использовать xargs (или parallel). Это очень мощная штука. 
    Обратите внимание, что вы можете контролировать количество команд на каждую строку (-L), а также параллельность (-P). Если вы не уверены, что делаете что-то правильно, начните с xargs echo. Еще -I{} – полезная штука. Примеры:

    Возможность объединения нескольких команд Linux в терминале и использования их в качестве конвейера, когда каждая следующая команда получает вывод предыдущей - очень мощный и гибкий инструмент. Но команды можно объединять не только так. 
    С помощью утилиты xargs вывод предыдущей команды можно передать в аргументы следующей.

    Синтаксис команды немного запутанный, но в нём можно разобраться:
    
    $ первая_команда | xargs опции вторая_команда аргументы
    
    Сначала выполняется любая первая команда и весь её вывод по туннелю передается в xargs. Затем этот вывод разбивается на строки и для каждой строки вызывается вторая команда, а полученная строка передаётся ей в аргументах.

      find . -name '*.py' | xargs grep some_function
      cat hosts | xargs -I{} ssh root@{} hostname

#### cd
    Перейти в домашнюю директорию можно с помощью cd. Для указания пути к файлам из домашней директории можно воспользоваться префиксом ~ (например, ~/.bashrc). В sh скриптах для обращения к домашней директории можно использовать переменную $HOME.
    Для того, чтобы перейти к предыдущей рабочей директории, используйте cd -

#### netstat

    Узнайте, какие процессы слушают порты через netstat -lntp или ss -plat (для TCP; добавьте -u для UDP).

#### history

    `Для просмотра последних команд используйте history. Повторить команду: !n (где n - порядковый номер истории). `

#### ssh

    Синтаксис команды выглядит следующим образом:

    $ ssh [опции] имя пользователя@сервер [команда]
    Чтобы просто подключиться к серверу по SSH:
    
    ssh user@host

    Мы привыкли подключаться к удаленному серверу, а уже потом выполнять нужные команды, 
    но на самом деле утилита ssh позволяет сразу выполнить нужную команду без открытия терминала удаленной машины. Например:
    
    ssh user@host ls

### Как найти файл с определенной подстрокой в директории со вложенностями? [&uarr;](#devmap)

    использовать find либо же grep

### Знакомы ли c `Source code management (SCM)` системами - `Git`, `Mercurial`, `SVN`, etc? [&uarr;](#devmap)

Наиболее популярны - Git, с большим отрывом идет SVN (Subversion) и Mercurial.
Долю на рынке, сопоставимую с массой электрона, занимают осталные системы : CVS (Concurrent Versions System), Team Foundation Server, Bazaar, Darcs итд.

Git и Mercurial — распределенные системы, SVN — централизованная.

В первом случае у каждого разработчика на локальной машине хранится полная копия репозитория, и он работает с ней автономно, периодически заливая обновления на сервер с главным репозиторием.

При этом Git, в отличие от Mercurial, работает не только с локальными коммитами, но и с локальными ветками, которые можно вовсе не заливать в удаленный репозиторий.

SVN предполагает, что полная версия кода со всеми ветками хранится в удаленном репозитории, а у разработчика локально находится только тот файл, который он сейчас модифицирует.

C этой точки зрения, SVN лучше вписывается в модель коммерческой разработки, где мы ежедневно контролируем объем и качество выполненной работы.

При использовании Git или Mercurial проектному менеджеру необходимо установить правило ежедневных коммитов для разработчиков, это особенно критично для удаленных команд или аутсорсеров.

### зачем нужны Source code management (SCM) [&uarr;](#devmap)

    Доступ к коду. Исходники кода хранятся в удаленном репозитории (хранилище данных), куда обращаются разработчики, чтобы забрать актуальную версию файлов или внести изменения. Так выстраивается командная разработка.
    
    Логирование изменений в коде. Отслеживание коммитов (внесений изменений в код), помогает найти кто, что и когда менял, решить конфликты при модифицировании одних и тех же файлов, откатиться на любое предыдущее состояние.
    
    Ветвление разработки. Программисты параллельно ведут разработку нового функционала в отдельных ветках, не затрагивая работоспособности старого.
    
    Поддержка версионности продуктов. При выпуске обновлений программных продуктов, мы обозначаем релизные версии, например, с помощью тегов, чтобы зафиксировать их в этом состоянии, для дебага или ретроспективы.

### какими пользовались [&uarr;](#devmap)

Git (Github, Gitlab, Bitbucket немного)

В целом, Bitbucket больше подходит для работы распределенных команд, а GitHub хорош для индивидуальных проектов.

Преимуществами Bitbucket в том, что в нем встроена интеграция с другими продуктами Atlassian. 
Если команда большая удобнее работать с Jira, если небольшая и задач немного, то подойдет и Trello.

### `gitflow`, `cherypick`, `revert` [&uarr;](#devmap)

Git-flow

    Git-flow — альтернативная модель ветвления Git, в которой используются функциональные ветки и несколько основных веток. Эта модель была впервые опубликована и популяризована Винсентом Дриссеном на сайте nvie. По сравнению с моделью магистральной разработки, в Git-flow используется больше веток, каждая из которых существует дольше, а коммиты обычно крупнее. В соответствии с этой моделью разработчики создают функциональную ветку и откладывают ее слияние с главной магистральной веткой до завершения работы над функцией. Такие долгосрочные функциональные ветки требуют тесного взаимодействия разработчиков при слиянии и создают повышенный риск отклонения от магистральной ветки. В них также могут присутствовать конфликтующие обновления.

    Git-flow можно использовать для проектов, в которых запланирован цикл релизов и реализуется характерная для DevOps методика непрерывной поставки. 
    В этом рабочем процессе используются понятия и команды, которые были предложены в рамках рабочего процесса с функциональными ветками. 
    Однако Git-flow привносит новые специфические роли для разных веток и определяет характер и частоту взаимодействия между ними. 
    Помимо функциональных веток в рамках этого рабочего процесса используются отдельные ветки для подготовки, поддержки и регистрации релизов. 
    При этом вы по-прежнему можете пользоваться преимуществами рабочего процесса с функциональными ветками, такими как запросы pull, изолированные эксперименты и эффективное командное взаимодействие.

git cherry-pick

    Команда git cherry-pick берёт изменения, вносимые одним коммитом, и пытается повторно применить их в виде нового коммита в текущей ветке. 
    Эта возможность полезна в ситуации, когда нужно забрать парочку коммитов из другой ветки, а не сливать ветку целиком со всеми внесенными в нее изменениями.

git revert

    Команда git revert — полная противоположность git cherry-pick. 
    Она создаёт новый коммит, который вносит изменения, противоположные указанному коммиту, по существу отменяя его.


### как перенести изменения из одной ветку в другую (2 способа) [&uarr;](#devmap)

merge или rebase

### зачем нужна команда `git rebase` [&uarr;](#devmap)

git rebase — это «автоматизированный» cherry-pick. Он выполняет ту же работу, но для цепочки коммитов, тем самым как бы перенося ветку на новое место.

это наложение коммитов поверх другого базового коммита. Под базовым понимается тот коммит, к которому применяются коммиты выбранной ветки.

Перебазирование в git используется для придания линейности истории ветки, чтобы удобно отслеживать изменения, или для обновления ветки разработки последними изменениями из основной ветки.

### Чем отличается `rebase` от `merge` - описание `rebase`, `merge`, `stash` [&uarr;](#devmap)

Git Merge

Слияние — обычная практика для разработчиков, использующих системы контроля версий. 
Независимо от того, созданы ли ветки для тестирования, исправления ошибок или по другим причинам, слияние фиксирует изменения в другом месте. 
Слияние принимает содержимое ветки источника и объединяет их с целевой веткой. 
В этом процессе изменяется только целевая ветка. История исходных веток остается неизменной.

    Плюсы:

    простота;
    сохраняет полную историю и хронологический порядок;
    поддерживает контекст ветки.


    Минусы:

    история коммитов может быть заполнена (загрязнена) множеством коммитов;
    отладка с использованием git bisect может стать сложнее.

Git Rebase

Rebase — еще один способ перенести изменения из одной ветки в другую. Rebase сжимает все изменения в один «патч». Затем он интегрирует патч в целевую ветку.

В отличие от слияния, перемещение перезаписывает историю, потому что она передает завершенную работу из одной ветки в другую. В процессе устраняется нежелательная история.

    Плюсы:

    Упрощает потенциально сложную историю
    Упрощение манипуляций с единственным коммитом
    Избежание слияния коммитов в занятых репозиториях и ветках
    Очищает промежуточные коммиты, делая их одним коммитом, что полезно для DevOps команд


    Минусы:

    Сжатие фич до нескольких коммитов может скрыть контекст
    Перемещение публичных репозиториев может быть опасным при работе в команде
    Появляется больше работы
    Для восстановления с удаленными ветками требуется принудительный пуш. Это приводит к обновлению всех веток, имеющих одно и то же имя, как локально, так и удаленно, и это ужасно.

    Если вы сделаете перемещение неправильно, история изменится, а это может привести к серьезным проблемам, поэтому убедитесь в том, что делаете!

Git squash

    Git squash — это прием, который помогает взять серию коммитов и уплотнить ее. Например, предположим: у вас есть серия из N коммитов и вы можете путем сжатия преобразовать ее в один-единственный коммит. 
    Сжатие через git squash в основном применяется, чтобы превратить большое число малозначимых коммитов в небольшое число значимых. Так становится легче отслеживать историю Git.

    Также этот прием используется при объединении ветвей. Чаще всего вам будут советовать всегда сжимать коммиты и выполнять перебазирование с родительской ветвью (например, master или develop). 
    В таком случае история главной ветки будет содержать только значимые коммиты, без ненужной детализации.

    сжатие коммитов меняет историю Git, поэтому не рекомендуется сжимать ветвь, если вы уже отправили ее в удаленный репозиторий. 
    Всегда выполняйте сжатие до того, как отправить пуш с изменениями.

    git rebase -i HEAD~3

### разница между `git` и `svn` (если есть) [&uarr;](#devmap)


    SVN – централизованная система контроля версий, Git – децентрализованная
    При ветвлении SVN копирует все содержимое ветки, Git – создает указатели

    Основное различие между этими двумя системами заключается в том, что Git — это распределенная система контроля версий, а Sagainst — централизованная система контроля версий. 
    Это означает, что вы держите репозиторий либо на своей машине (распределенный), так что вы можете работать локально, а затем синхронизировать изменения с общим сервером. В SVN весь код размещается в одном месте, и все разработчики должны быть подключены к нему, чтобы каждый мог синхронизировать и загружать изменения с сервера

### зачем нужна [&uarr;](#devmap)

Задачи системы управления проектами
Поддержка планирования проектов (планирование сроков, ресурсов, финансов). Поддержка принятия решений по реализации проектов компании. Контроль реализации проектов компании. Контроль использования в проектах финансовых, трудовых и материальных ресурсов компании.


### какими пользовались [&uarr;](#devmap)

Redmine - для рабочих проектов
Trello - для личных проектов

### как была организована работа в команде [&uarr;](#devmap)

работа велась по методике Waterfall (каскадная модель)

    Waterfall — методика управления проектами, которая подразумевает последовательный переход с одного этапа на другой без пропусков и возвращений на предыдущие стадии. 

    Agile — система идей и принципов «гибкого» управления проектами, на основе которых разработаны популярные методы Scrum, Kanban и другие. Ключевой принцип — разработка через короткие итерации (циклы), в конце каждого из которых заказчик (пользователь) получает рабочий код или продукт. 

Agile стал основой для целого ряда гибких методик, среди которых наиболее известны Scrum, Lean и экстремальное программирование.

    Scrum — методология гибкой разработки на основе Agile, в основе которого лежит «спринт» — отрезок от 1 до 4 недель, по окончанию которого должна быть получена рабочая версия продукта.

    Lean — метод, который вырос на основе системы управления производством Toyota Production System. В его основе — философия постоянного совершенствования на всех уровнях организации, где одно из ключевых понятий — ценность (то, за что готов платить заказчик).

    Экстремальное программирование (XP) — одна из Agile-методик, где важная роль отводится периодической игре в планирование с привлечением заказчика. Она позволяет определить недостатки предыдущей итерации, приоритетность задач, желаемую функциональность продукта с учётом пожеланий заказчика.

Как я считаю, agile лучше использовать в разработке а waterfall в поддержке.


### методы разрешения конфликтов [&uarr;](#devmap)

    Существует несколько основные проявления в коллективе:

    разногласия между личностью и личностью (межличностный),
    между личностью и группой,
    противостояние групповое (межгрупповой). 

    Основными эмоциональными причинами проявления, проявлениями совместной деятельности, выраженной конфликтогенностью взаимодействия, ощущениями ощущений, проявлениями чувств противоречиями, эмоциональными манипуляциями и проявлениями, выявлением проявлений, сознательным выявлением проявлений, столкновением интересов и пр.


    Основные навыки успешного управления конфликтами

    Определить источник конфликта – взвешенный взгляд людей на одну и ту же проблему, расхождение в частностях, ценностях и назначениях.
    Установить связь конфликтующих сторон – поиск общности целей и желание пойти на взаимные уступки.
    Наладить конструктивный диалог – выявление агрессии, резких эмоций и эмоций, встречать диалог на взаимном уважении. Стремиться к заключению выгодного договора для сторон.
    Создать позитивные связи – чем быстрее наладить коммуникацию, тем быстрее сосредоточиться на спорных вопросах и достижении целей.
### понимание методологии `Scrum` и ее атрибутов (стендапы, грумминг/оценка бэклога, ретроспективы) [&uarr;](#devmap)

В настоящее время, Scrum является одной из наиболее популярных «методологий» разработки ПО. Согласно определению, Scrum — это каркас разработки, с использованием которого люди могут решать появляющиеся проблемы, при этом продуктивно и производя продукты высочайшей значимости

В классическом Scrum существует 3 базовых роли:
-Product owner
-Scrum master
-Команда разработки (Development team)

Product owner (PO) является связующим звеном между командой разработки и заказчиком. Задача PO — максимальное увеличение ценности разрабатываемого продукта и работы команды.

Одним из основных инструментов PO является Product Backlog. Product Backlog содержит необходимые для выполнения рабочие задачи (такие как Story, Bug, Task и др.), отсортированные в порядке приоритета (срочности).

Scrum master (SM) является «служащим лидером» (англ. servant-leader). Задача Scrum Master — помочь команде максимизировать ее эффективность посредством устранения препятствий, помощи, обучении и мотивации команде, помощи PO

Команда разработки (Development team, DT) состоит из специалистов, производящих непосредственную работу над производимым продуктом. 

Рекомендуемый размер команды — 7 (плюс-минус 2) человека. Согласно идеологам Scrum, команды большего размера требуют слишком больших ресурсов на коммуникации, в то время как команды меньшего размера повышают риски (за счет возможного отсутствия требуемых навыков) и уменьшают размер работы, который команда может выполнить в единицу времени.

Процесс Scrum

Основой Scrum является Sprint, в течении которого выполняется работа над продуктом. По окончанию Sprint должна быть получена новая рабочая версия продукта. Sprint всегда ограничен по времени (1-4 недели) и имеет одинаковую продолжительность на протяжении всей жизни продукта.

Перед началом каждого Sprint производится Sprint Planning, на котором производится оценка содержимого Product Backlog и формирование Sprint Backlog, который содержит задачи (Story, Bugs, Tasks), которые должны быть выполнены в текущем спринте. Каждый спринт должен иметь цель, которая является мотивирующим фактором и достигается с помощью выполнения задач из Sprint Backlog.

Каждый день производится Daily Scrum, на котором каждый член команды отвечает на вопросы «что я сделал вчера?», «что я планирую сделать сегодня?», «какие препятствия на своей работе я встретил?». Задача Daily Scrum — определение статуса и прогресса работы над Sprint, раннее обнаружение возникших препятствий, выработка решений по изменению стратегии, необходимых для достижения целей Sprint'а.

По окончанию Sprint'а производятся Sprint Review и Sprint Retrospective, задача которых оценить эффективность (производительность) команды в прошедшем Sprint'е, спрогнозировать ожидаемую эффективность (производительность) в следующем спринте, выявлении имеющихся проблем, оценки вероятности завершения всех необходимых работ по продукту и другое.

# Кэширование

Кэширование (или кэш) – это некий промежуточный буфер, в котором хранятся данные. Благодаря кэшированию страница сайта не воссоздается заново для каждого пользователя. Кэширование позволяет осуществлять работу с большим количеством данных в максимально сжатые сроки и при ограниченных ресурсах (серверных и пользовательских).

### какие виды кэширования использовал [&uarr;](#devmap)

Сайт может хранить данные для ускорения обработки последующих запросов на четырёх уровнях:

    клиентский;
    сетевой;
    серверный;
    уровень приложения.

1. Браузерное кэширование или клиентское кэширование

    Заголовки HTTP отвечают за определение возможности кэширования ответа и за определение срока хранения данных. 
    
    Cache-control -  указывает, скролько по времени ответ может находиться в кэше. Браузер отправит повторный запрос на хранение данных, если срок хранения истечёт или пользователь целенаправленно обновит страницу.

    Last-Modified или Etag - эти заголовки нужны для проверки возможности повторного использования данных. Статус ответа 304 указывает, что содержимое не изменилось и повторная загрузка не требуется.

    Сайт с продуманными HTTP-заголовками обретёт больший успех у пользователей. Кроме того, браузер сэкономит время и пропускную способность.

2) Кэш на сетевом уровне

    Согласно Википедии, Сеть Доставки Контента (CDN) — географически распределённая сетевая инфраструктура, позволяющая оптимизировать доставку и дистрибуцию контента конечным пользователям в сети Интернет. Иначе говоря, CDN — это распределённое хранение и использование кэша.

    Директива HTTP-заголовка Cache-control: public позволяет различным частям сети кэшировать ответ. С помощью заголовка Cache-Control: public, max-age=31536000 находят ресурсы, которые хранятся в течение одного года.


3) Кэш на серверном уровне

OPcache, XCache и тд.

Кэш на уровне приложения

PHP и MYSQL кэш.


### `Memcache`, `Redis` [&uarr;](#devmap)
### Если использовал `Memcache`, то с какими проблемами сталкивался. [&uarr;](#devmap)

# Тесты

### зачем нужны [&uarr;](#devmap)
### функциональные, стресс, юнит тесты [&uarr;](#devmap)
### что такое `TDD` [&uarr;](#devmap)

# Web специфика

### виды сетевых протоколов [&uarr;](#devmap)

#### IP — Internet Protocol

Протокол передачи, который первым объединил отдельные компьютеры в единую сеть. Самый примитивный в этом списке. Он является ненадёжным, т. е. не подтверждает доставку пакетов получателю и не контролирует целостность данных. По протоколу IP передача данных осуществляется без установки соединения.

Основная задача этого протокола — маршрутизация датаграмм, т. е. определение пути следования данных по узлам сети.

Популярная версия на текущий момент — IPv4 с 32-битными адресами. Это значит, что в интернете могут хранится 4.29 млрд адресов IPv4. Число большое, но не бесконечное. Поэтому существует версия IPv6, которая поможет решить проблему переполнения адресов, ведь уникальных IPv6 будет 2 ^ 128 адресов (число с 38 знаками).

#### TCP/IP — Transmission Control Protocol/Internet Protocol

Это стек протоколов TCP и IP. Первый обеспечивает и контролирует надёжную передачу данных и следит за её целостностью. Второй же отвечает за маршрутизацию для отправки данных. Протокол TCP часто используется более комплексными протоколами.
UDP — User Datagram Protocol

Протокол, обеспечивающий передачу данных без предварительного создания соединения между ними. Этот протокол является ненадёжным. В нём пакеты могут не только не дойти, но и прийти не по порядку или вовсе продублироваться.

Основное преимущество UDP протокола заключается в скорости доставки данных. Именно поэтому чувствительные к сетевым задержкам приложения часто используют этот тип передачи данных.

#### FTP — File Transfer Protocol

Протокол передачи файлов. Его использовали ещё в 1971 году — задолго до появления протокола IP. На текущий момент этим протоколом пользуются при удалённом доступе к хостингам. FTP является надёжным протоколом, поэтому гарантирует передачу данных.

Этот протокол работает по принципу клиент-серверной архитектуры. Пользователь проходит аутентификацию (хотя в отдельных случаях может подключаться анонимно) и получает доступ к файловой системе сервера.
DNS

Это не только система доменных имён (Domain Name System), но и протокол, без которого эта система не смогла бы работать. Он позволяет клиентским компьютерам запрашивать у DNS-сервера IP-адрес какого-либо сайта, а также помогает обмениваться базами данных между серверами DNS. В работе этого протокола также используются TCP и UDP.

#### HTTP — HyperText Transfer Protocol

Изначально протокол передачи HTML-документов. Сейчас же он используется для передачи произвольных данных в интернете. Он является протоколом клиент-серверного взаимодействия без сохранения промежуточного состояния. В роли клиента чаще всего выступает веб-браузер, хотя может быть и, например, поисковый робот. Для обмена информацией протокол HTTP в большинстве случаев использует TCP/IP.

HTTP имеет расширение HTTPS, которое поддерживает шифрование. Данные в нём передаются поверх криптографического протокола TLS.

#### NTP — Network Time Protocol

Не все протоколы передачи нужны для обмена классического вида информацией. NTP — протокол для синхронизации локальных часов устройства со временем в сети. Он использует алгоритм Марзулло. Благодаря нему протокол выбирает более точный источник времени. NTP работает поверх UDP — поэтому ему удаётся достигать большой скорости передачи данных. Протокол достаточно устойчив к изменениям задержек в сети.

Последняя версия NTPv4 способна достигать точности 10мс в интернете и до 0,2мс в локальных сетях.

#### SSH — Secure SHell

Протокол для удалённого управления операционной системой с использованием TCP. В SSH шифруется весь трафик, причём с возможностью выбора алгоритма шифрования. В основном это нужно для передачи паролей и другой важной информации.

Также SSH позволяет обрабатывать любые другие протоколы передачи. Это значит, что кроме удалённого управления компьютером, через протокол можно пропускать любые файлы или даже аудио/видео поток.

SSH часто применяется при работе с хостингами, когда клиент может удалённо подключиться к серверу и работать уже оттуда.

### Что такое `https` и зачем он нужен [&uarr;](#devmap)

HTTPS (от англ. HyperText Transfer Protocol Secure) – расширение протокола HTTP, которое используется для шифрования и безопасного обмена данными между пользователем и сайтом. Вся информация шифруется с помощью криптографических протоколов TSL или его предшественника SSL. Википедия рулит! Если не уходить глубоко в технические подробности, то HTTPS шифрует данные и не дает возможности их перехватить.


Принципом работы защищённого протокола HTTPS является обмен ключами шифрования. Прежде чем ответить на запрос от браузера, сервер предъявляет ключ — SSL-certificate. Браузер проверяет подлинность ключа в Центре сертификации. Если ключ «подошёл», браузер и сервер доверяют друг другу и договариваются о разовом шифре. Так происходит каждую сессию, то есть каждый раз при обмене запросами и ответами. Вот таким хитрым способом и обеспечивается сохранность данных и конфиденциальность при обмене информацией.

Как это работает? Как только Вы зашли на сайт, то Ваш браузер получает от сервера сертификат, в котором есть вся информация о сайте, его владельце, кем выдан и еще много всего. Самое главное, Ваш браузер получает открытый ключ, по которому он начнет шифровать все данные. На сервере стоит закрытый ключ, с помощью которого можно расшифровать всю информацию.

Если кто-то захочет перехватить или получить конфиденциальные данные, то он сможет увидеть только набор с непонятных символов, на расшифровку которых потребуется несколько лет. За это время сертификат устареет, и в итоге злоумышленник ничего не получит.

#### Зачем нужен SSL-сертификат для сайта

    Чтобы сайт стал работать по протоколу безопасного соединения HТТPS, нужен SSL-сертификат. Это виртуальный документ, который содержит данные об организации, её владельце и подтверждает их существование. Позволяет узнать сервер и подтвердить безопасность сайта.

    Использование сертификата безопасности для сайта гарантирует:

        Подлинность ресурса, к которому обращается пользователь. Это повышает у посетителей уровень доверия.
        Целостность передаваемой информации. При транспортировке от сервера к браузеру данные не изменятся и не потеряются.
        Конфиденциальность. 256-разрядное шифрование исключает доступ злоумышленников к информации.

    Что дает SSL-сертификат для сайта кроме защиты данных? SSL-сертификат помогает в SEO-продвижении проекта — позволяет занять более высокую позицию в поисковой выдаче. Поисковые системы (Google, Яндекс и пр.) дорожат доверием аудитории и выше ранжируют сайты, которые работают через безопасное соединение.


### Понимание базовых аспектов функционирования сети - протоколы, `DNS`, и т.д.; [&uarr;](#devmap)

Интернет – это глобальная информационная система, которая:

·        логически взаимосвязана пространством глобальных уникальных адресов, основанных на Интернет-протоколе (IP);

·        способна поддерживать коммуникации с использованием семейства протокола управления передачей - TCP/IP или его последующих расширений/преемников и/или других IP-совместимых протоколов;

·         обеспечивает, использует или делает доступными на общественной или частной основе высокоуровневые услуги, надстроенные над описанной здесь коммуникационной и иной связанной с ней инфраструктурой.

 

Инфраструктура Интернет:

1.магистральный уровень (система связанных высокоскоростных телекоммуникационных серверов).

2.уровень сетей и точек доступа (крупные телекоммуникационные сети), подключенных к магистрали.

3.уровень региональных и других сетей.

4.ISP – интернет-провайдеры.

5.пользователи.


К техническим ресурсам сети Интернет относятся компьютерные узлы, маршрутизаторы, шлюзы, каналы связи и др.

### Что происходит при открытии вкладки браузера [&uarr;](#devmap)


#### 1. Пользователь вводит в браузере адрес сайта

#### 2. Браузер начинает искать сервер

За работу любого сайта обычно отвечает один из миллионов серверов, подключенных к интернету. 
Адрес сервера — это уникальный набор цифр, который называется IP-адресом. 
Например, для vc.ru— это сервер 85.119.149.83.

Поэтому первым делом браузеру нужно понять, какой IP-адрес у сервера, на котором находится сайт.

Такая информация хранится в распределенной системе серверов — DNS (Domain Name System). 
Система работает как общая «контактная книга», хранящаяся на распределенных серверах и устройствах в интернете.

Однако перед тем, как обращаться к DNS, браузер пытается найти запись об IP-адресе сайта в ближайших местах, чтобы сэкономить время:

    Сначала в своей истории подключений. Если пользователь уже посещал сайт, то в браузере могла сохраниться информация c IP-адресом сервера.
    В операционной системе. Не обнаружив информации у себя, браузер обращается к операционной системе, которая также могла сохранить у себя DNS-запись. Например, если подключение с сайтом устанавливалось через одно из установленных на компьютере приложений.
    В кэше роутера, который сохраняет информацию о последних соединениях, совершенных из локальной сети.

#### 3. Браузер отправляет запрос к DNS-серверам

Не обнаружив подходящих записей в кэше, браузер формирует запрос к DNS-серверам, расположенным в интернете.

Например, если нужно найти IP-адрес сайта mail.vc.ru, браузер спрашивает у ближайшего DNS-сервера «Какой IP-адрес у сайта mail.vc.ru?».

Сервер может ответить: «Я не знаю про mail.vc.ru, но знаю сервер, который отвечает за vc.ru». Запрос переадресовывается дальше, на сервер «выше», пока в итоге один из серверов не найдет ответ об IP-адресе для сайта.

#### 4. Браузер устанавливает соединение с сервером

Как только браузер узнал IP-адрес нужного сервера, он пытается установить с ним соединение. В большинстве случаев для этого используется специальный протокол — TCP.

TCP — это набор правил, который описывает способы соединения между устройствами, форматы отправки запросов, действия в случае потери данных и так далее.

Например, для установки соединения между браузером и сервером в стандарте TCP используется система «трёх рукопожатий». Работает она так:

    Устройство пользователя отправляет специальный запрос на установку соединения с сервером — называется SYN-пакет.
    Сервер в ответ отправляет запрос с подтверждением получения SYN-пакета — называется SYN/ACK-пакет.
    В конце устройство пользователя при получении SYN/ACK-пакета отправляет пакет с подтверждением — ACK-пакет. В этот момент соединение считается установленным.

#### 5. Браузер отправляет HTTP-запрос, чтобы получить контент сайта

После установки соединения браузер отправляет специальный запрос, в котором просит сервер отправить данные для отображения страницы. В этом запросе содержится информация о самом браузере, временные файлы, требования к соединению и так далее.

Задача браузера — как можно подробнее объяснить серверу, какая именно информация ему нужна.

В общении браузера и сервера выделяют два типа запросов. GET-запрос используется для получения данных с сервера — например, отобразить картинку, текст или видео. POST-запрос — используется для отправки данных из браузера на сервер, например, когда пользователь отправляет сообщение, картинку или загружает файл.

    Почти все сайты обмениваются информацией с сервером в зашифрованном формате — с помощью HTTPS-протокола. 
    В отличие от HTTP-протокола, в HTTPS используется шифрование, а безопасность подключения подтверждается специальным сертификатом.


#### 6. Сервер обрабатывает запрос

Сервер получил запрос от браузера с подробным описанием того, что ему требуется. 
Теперь ему нужно обработать этот запрос. Этой задачей занимается специальное серверное программное обеспечение — например, nginx или Apache. 
Чаще всего такие программы принято называть веб-серверами.

Веб-сервер в свою очередь перенаправляет запрос на дальнейшую обработку к программе-обработчику — например, PHP, Ruby или ASP.NET. 
Программа внимательно изучает содержимое запроса — например, понимает, в каком формате нужно отправить ответ и какие именно файлы нужны. И собирает ответ.

#### 7. Сервер отправляет ответ браузеру

Когда ответ сформирован, он отправляется веб-сервером обратно браузеру. В ответе как правило содержится контент для отображения веб-страницы, информация о типе сжатия данных, способах кэширования, файлы cookie, которые нужно записать и так далее.
    
    👉 Чтобы обмен данными был быстрым, браузер и сервер обмениваются сразу множеством небольших пакетов данных — как правило, в пределах 8 КБ. 
    Все пакеты имеют специальные номера, которые помогают отслеживать последовательность отправки и получения данных.

#### 8. Браузер обрабатывает полученный ответ и «рисует» веб-страницу

Браузер распаковывает полученный ответ и постепенно начинает отображать полученный контент на экране пользователя — этот процесс называется рендерингом.

Сначала браузер загружает только основную структуру HTML-страницы. Затем последовательно проверяет все теги и отправляет дополнительные GET-запросы для получения с сервера различных элементов — картинки, файлы, скрипты, таблицы стилей и так далее. Поэтому по мере загрузки страницы браузер и сервер продолжают обмениваться между собой информацией.

Параллельно с этим на компьютер как правило сохраняются статичные файлы пользователя — чтобы при следующем посещении не загружать их заново и быстрее отобразить пользователю содержимое страницы.

Как только рендеринг завершен — пользователю отобразится полностью загруженная страница сайта.




### C каким серверным ПО приходилось работать? [&uarr;](#devmap)

Веб-сервер - Apache, Nginx. Веб-сервер — это специальная программа, которая принимает запросы пользователей, обрабатывает их и отправляет ответ обратно по протоколу прикладного уровня HTTP.

Интерпретатор языка программирования

СУБД — система управления базами данных: MySQL, PostgreSQL, MS SQL, Oracle, Redis, MongoDB и т.д.

Поисковые системы — ElasticSearch / Sphinx — позволяют осуществлять поиск и фильтрацию быстрее, нежели это возможно с использованием реляционных СУБД. Kibana - для визуализации данных, полученных из Elasticsearch - это в рамках настройки хранилища для логов на базе Elasticsearch, Logstash и Kibana, которое называют ELK Stack. Sphinx - движок полнотекстового поиска.

Elasticsearch (далее ES) — масштабируемая поисковая система, которую также можно отнести к нереляционным (noSQL) базам данных. В основном используется для полнотекстового поиска с фильтрами и анализаторами.


Кеширующие сервера — системы, «запоминающие» результат обработки запросов и использующие эти данные при повторных обращениях для ускорения генерации страниц — Memcached и Redis.

Софт для резервного копирования — бэкапы должны создаваться регулярно и автоматически, а также хранится не на том же сервере, где расположены «боевые данные».

Ускорители исполнения программного кода. Служат для повышения производительности, часто используемые ускорители для PHP: APC, eAccellerator, XCache.

Мониторинг и оповещения — системы, собирающие важные метрики производительности системы и сообщающие о проблемах.

### Что такое `Apache` и `mod_rewrite`? [&uarr;](#devmap)

Apache – это свободное программное обеспечение для размещения веб-сервера. Он хорошо показывает себя в работе с масштабными проектами.

mod_rewrite — это модуль для веб-сервера Apache, предназначенный для преобразования URL-ов. Модуль использует в своей работе правила, которые могут быть описаны как в конфигурации сервера (httpd.conf), так и в файлах .htaccess непосредственно в файловой структуре Вашего сайта. Правила описываются в виде регулярных выражений 

### `nginx`, его отличие от `apache` [&uarr;](#devmap)

#### Краткий обзор Apache

Apache был разработан для доставки веб-контента, доступ к которому осуществляется через Интернет. Он известен тем, что играл ключевую роль в начальном росте интернета. Apache - это программное обеспечение с открытым исходным кодом, разработанное и поддерживаемое открытым сообществом разработчиков и работающее в самых разных операционных системах. Архитектура включает в себя ядро Apache и модули. Основной компонент предоставляет базовую серверную функцию, поэтому он принимает соединения и управляет параллелизмом. Различные модули соответствуют различным функциям, которые выполняются по каждому запросу. Конкретное развертывание Apache может быть сконфигурировано для включения различных модулей, таких как функции безопасности, управление динамическим контентом или для базовой обработки HTTP-запросов.

Модель «один сервер делает все» стала ключом к раннему успеху Apache. Однако по мере увеличения уровня трафика и увеличения количества веб-страниц и ограничения производительности настройка Apache на работу с реальным трафиком усложнялась.

#### Краткий обзор Nginx

Nginx был разработан специально для устранения ограничений производительности веб-серверов Apache. Производительность и масштабируемость Nginx обусловлены архитектурой, управляемой событиями. Он значительно отличается от подхода Apache. В Nginx каждый рабочий процесс может одновременно обрабатывать тысячи HTTP-соединений. Следовательно, Nginx - это легковесная, масштабируемая и высокопроизводительная реализация. Эта архитектура делает обработку больших и флуктуирующих нагрузок на данные гораздо более предсказуемой с точки зрения использования ОЗУ, использования ЦП и задержки.

Nginx также имеет богатый набор функций и может выполнять различные роли сервера:

    Обратный прокси-сервер для протоколов HTTP, HTTPS, SMTP, POP3 и IMAP
    Балансировщик нагрузки и HTTP-кеш
    Интерфейсный прокси для Apache и других веб-серверов, сочетающий гибкость Apache с хорошей производительностью статического контента Nginx

#### Apache против Nginx: сравнение их богатых наборов функций
#### Простота

Разрабатывать и обновлять приложения на Apache очень просто. Модель «одно соединение на процесс» позволяет очень легко вставлять модули в любой точке логики веб-обслуживания. Разработчики могут добавлять код таким образом, что в случае сбоев будет затронут только рабочий процесс, выполняющий код. Обработка всех других соединений будет продолжаться без помех.

Nginx, с другой стороны, имеет сложную архитектуру, поэтому разработка модулей не легка. Разработчики модулей Nginx должны быть очень осторожны, чтобы создавать эффективный и точный код, без сбоев, и соответствующим образом взаимодействовать со сложным ядром, управляемым событиями, чтобы избежать блокирования операций.

#### Производительность

Производительность измеряется тем, как сервер доставляет большие объемы контента в браузер клиента, и это важный фактор. Контент может быть статическим или динамическим. Давайте посмотрим статистику по этому вопросу.
Статический контент

Nginx работает в 2,5 раза быстрее, чем Apache, согласно тесту производительности, выполняемому до 1000 одновременных подключений. Другой тест с 512 одновременными подключениями показал, что Nginx примерно в два раза быстрее и потребляет меньше памяти. Несомненно, Nginx имеет преимущество перед Apache со статическим контентом. Поэтому, если вам нужно обслуживать одновременный статический контент, Nginx является предпочтительным выбором.

#### Динамический контент

Результаты тестов Speedemy показали, что для динамического контента производительность серверов Apache и Nginx была одинаковой. Вероятная причина этого заключается в том, что почти все время обработки запросов расходуется в среде выполнения PHP, а не в основной части веб-сервера. Среда выполнения PHP довольно похожа для обоих веб-серверов.

Apache также может обрабатывать динамический контент, встраивая процессор языка, подобного PHP, в каждый из его рабочих экземпляров. Это позволяет ему выполнять динамический контент на самом веб-сервере, не полагаясь на внешние компоненты. Эти динамические процессоры могут быть включены с помощью динамически загружаемых модулей.

Nginx не имеет возможности обрабатывать динамический контент изначально. Чтобы обрабатывать PHP и другие запросы на динамический контент, Nginx должен перейти на внешний процессор для выполнения и дождаться отправки визуализированного контента. Однако этот метод также имеет некоторые преимущества. Поскольку динамический интерпретатор не встроен в рабочий процесс, его издержки будут присутствовать только для динамического содержимого.

#### Поддержка ОС

Apache работает во всех операционных системах, таких как UNIX, Linux или BSD, и полностью поддерживает Microsoft Windows. Nginx также работает на нескольких современных Unix-подобных системах и поддерживает Windows, но его производительность в Windows не так стабильна, как на платформах UNIX.
Безопасность

И Apache, и Nginx являются безопасными веб-серверами. Apache Security Team существует, чтобы предоставить помощь и советы проектам Apache по вопросам безопасности и координировать обработку уязвимостей безопасности. Важно правильно настроить серверы и знать, что делает каждый параметр в настройках.

#### Гибкость

Веб-серверы могут быть настроены путем добавления модулей. Apache долго загружал динамические модули, поэтому все модули Apache поддерживают это.

Nginx Plus (Nginx Plus - это программный балансировщик нагрузки, веб-сервер и кэш контента, построенный на основе открытого исходного кода Nginx) также использует модульную архитектуру. Новые функции и возможности могут быть добавлены с программными модулями, которые могут быть подключены к работающему экземпляру Nginx Plus по требованию. Динамические модули добавляют в Nginx Plus такие функции, как геолокация пользователей по IP-адресу, изменение размеров изображений и встраивание сценариев Lua в модель обработки событий Nginx Plus. Модули создаются как Nginx, Inc., так и сторонними разработчиками.

Большинство необходимых функциональных возможностей основного модуля (например, прокси, кэширование, распределение нагрузки) поддерживаются обоими веб-серверами.

#### Поддержка и документация

Важным моментом, который следует учитывать, является доступная справка и поддержка веб-серверов среди прочего программного обеспечения. Поскольку Apache был популярен так долго, поддержка сервера довольно распространена повсеместно. Для главного сервера и для основанных на задачах сценариев, связанных с подключением Apache к другому программному обеспечению, имеется большая библиотека документации первого и стороннего производителя.

Наряду с документацией многие инструменты и веб-проекты содержат инструменты для начальной загрузки в среде Apache. Это может быть включено в сами проекты или в пакеты, поддерживаемые отделом упаковки вашего дистрибутива.

Apache, как правило, получает большую поддержку от сторонних проектов просто из-за своей доли рынка и продолжительности времени, в течение которого он был доступен.

В прошлом для Nginx было трудно найти исчерпывающую англоязычную документацию из-за того, что большая часть ранней разработки и документации была на русском языке. Однако на сегодняшний день документация заполнена, и на сайте Nginx имеется множество ресурсов для администрирования и доступной документации от третьих лиц. 

#### Nginx и Apache - Совместная работа

Для многих приложений Nginx и Apache хорошо дополняют друг друга. Очень распространенным начальным шаблоном является развертывание программного обеспечения Nginx с открытым исходным кодом в качестве прокси-сервера (или Nginx Plus в качестве платформы доставки приложений) перед веб-приложением на основе Apache. Nginx выполняет тяжелую работу, связанную с HTTP - обслуживает статические файлы, кэширует содержимое и разряжает медленные HTTP-соединения - так что сервер Apache может выполнять код приложения в безопасной и защищенной среде. 


### балансировка нагрузки на сервера приложений (`haproxy`) [&uarr;](#devmap)

HAProxy, или High Availability Proxy, является программным балансировщиком нагрузки TCP/HTTP. Он распределяет рабочую нагрузку по серверам для обеспечения максимальной производительности и оптимизации использования ресурсов. HAProxy поддерживает гибко настраиваемые методы проверки доступности, обработки отказов и восстановления после них.

HAProxy устанавливается на отдельный сервер, который принимает клиентские запросы и перенаправлять их на веб-сервера Nginx. 

### работы с системами очередей (`RabbitMQ`, `Kafka`) [&uarr;](#devmap)

#### Apache Kafka

Apache Kafka – это распределенная платформа потоковой передачи событий с открытым исходным кодом, обеспечивающая высокую пропускную способность. Написанная на Java и Scala, Кафка представляет собой шину сообщений системы Pub/Sub, ориентированную на потоки и воспроизведение данных с высокой интенсивностью. Кафка не полагается на очередь, а добавляет сообщения в журнал и оставляет их там до достижения предела хранения или тех пор, пока консьюмер не прочитает эти сообщения.

Apache Kafka лучше всего подходит для потоковой передачи от А к Б без сложной маршрутизации, но с максимальной пропускной способностью. Инструмент отлично справляется с event sourcing, потоковой обработкой и моделированием изменений в системе в качестве последовательности событий. Кафку также можно использовать для обработки данных при многоэтапной конвейерной обработке.

Кафка станет отличным решением, если вам нужен фреймворк для хранения, чтения, повторного чтения и анализа потоковых данных. Ее сильная сторона – обработка и анализ данных в реальном времени. Инструмент идеально подходит для постоянного хранения сообщений или для регулярно проверяемых систем.

#### RabbitMQ

RabbitMQ – это распределенный брокер сообщений с открытым исходным кодом, который обеспечивает эффективную доставку сообщений в рамках сложных сценариев маршрутизации. Этот инструмент называется «распределенным», потому что обычно работает как кластер узлов, где очереди распределяются (реплицируются) по узлам для обеспечения высокой доступности и отказоустойчивости.

По умолчанию в RabbitMQ используется протокол AMQP 0.9.1, также существуют расширения для поддержки дополнительных протоколов: AMQP 1.0, HTTP, STOMP и MQTT. RabbitMQ официально поддерживает Elixir, Go, Java, JavaScript, .NET, PHP, Python, Ruby, Objective-C, Spring и Swift. Пользователям доступны различные инструменты разработки и клиенты, использующие расширения сообщества.

Разработчики используют RabbitMQ для обработки высокопроизводительных и надежных фоновых заданий, а также для интеграции и взаимодействия внутри приложений и между ними. Инструмент применяется для выполнения сложной маршрутизации к консьюмерам и интеграции нескольких приложений и служб с нетривиальной логикой маршрутизации.

RabbitMQ идеально подходит для веб-серверов, которым требуется быстрый запрос-ответ. Этот инструмент распределяет нагрузку между рабочими приложениями при высокой нагрузке (более 20 000 сообщений в секунду) и может обрабатывать фоновые задания или длительные задачи, такие как преобразование PDF, сканирование файлов или масштабирование изображений.

#### Основные отличия Apache Kafka и RabbitMQ

    Поток данных. RabbitMQ использует определенный ограниченный поток данных. Продюсер создает и отправляет сообщения, а консьюмер их принимает. Apache Kafka использует неограниченный поток данных, при этом пары «ключ-значение» непрерывно передаются в назначенную тему.

    Использование данных. RabbitMQ отлично подходит для запросов пользователей и транзакционных данных, таких как создание и размещение заказов. Кафка лучше справляется с операционными данными, такими как технологические процессы, статистика аудита и сбора данных, активность системы.

    Обмен сообщениями. RabbitMQ отправляет пользователям сообщения, которые удаляются из очереди после их обработки и подтверждения. Кафка – это журнал. Он использует непрерывные цепочки сообщений, которые сохраняются в очереди до истечения срока хранения.

    Модель проектирования. RabbitMQ использует модель умный брокер/тупой консьюмер. Брокер последовательно доставляет сообщения консьюмерам и отслеживает их статус. Apache Kafka использует модель тупого брокера/умного консьюмера. Этот инструмент не отслеживает сообщения, которые прочитал каждый пользователь. Кафка запоминает только непрочитанные сообщения, сохраняя их в течение установленного периода времени. Консьюмеры должны самостоятельно следить за своей позицией в каждом журнале.

    Топология. RabbitMQ использует топологию обмена очереди: сообщения отправляются на обмен, откуда затем рассылаются в различные привязки очередей для использования консьюмерами. Кафка использует топологию Publish/Subscribe, отправляя сообщения через поток в соответствующие топики, которые затем потребляются пользователями в разных авторизованных группах.

### `CI` (`Continuous Integration`) [&uarr;](#devmap)

CI/CD означает «Continuous Integration/Continuous Delivery» — то есть «непрерывная интеграция/непрерывная доставка». Это подход к разработке, при котором задачи сборки, публикации, тестирования продукта полностью или частично автоматизированы. Очень часто автоматизация интегрирована в бизнес-процессы продуктовой команды или компании, но практики CI/CD прекрасно могут быть внедрены и в проекты, в которых участвует только один разработчик.

Существует большое количество инструментов для сборки кода и публикации его для пользователей. У каждого свои особенности и тонкости использования. Но есть и конкурирующие между собой инструменты. Среди конкурирующих платформ стоит говорить о GitLab CI/CD, Bitbucket Pipelines, Jenkins, Netlify, JetBrains TeamCity, GitHub Actions и прочие.

Здесь я кратко и по возможности без огрехов опишу процесс работы системы с высоты птичьего полёта:

    разработчик отпраляет коммит в репозиторий, создаёт merge request через сайт, или ещё каким-либо образом явно или неявно запускает пайплайн,
    из конфигурации выбираются все задачи, условия которых позволяют их запустить в данном контексте,
    задачи организуются в соответствии со своими этапами,
    этапы по очереди выполняются — т.е. параллельно выполняются все задачи этого этапа,
    если этап завершается неудачей (т.е. завершается неудачей хотя бы одна из задач этапа) — пайплайн останавливается (почти всегда),
    если все этапы завершены успешно, пайплайн считается успешно прошедшим.


Таким образом, имеем:

    пайплайн — набор задач, организованных в этапы, в котором можно собрать, протестировать, упаковать код, развернуть готовую сборку в облачный сервис, и пр.,
    этап (stage) — единица организации пайплайна, содержит 1+ задачу,
    задача (job) — единица работы в пайплайне. Состоит из скрипта (обязательно), условий запуска, настроек публикации/кеширования артефактов и много другого.


Соответственно, задача при настройке CI/CD сводится к тому, чтобы создать набор задач, реализующих все необходимые действия для сборки, тестирования и публикации кода и артефактов.

### Деплой [&uarr;](#devmap)

Деплой (deploy) — это развертывание и запуск веб-приложения или сайта в его рабочей среде, то есть на сервере или хостинге. Разработчик загружает приложение, написанное на локальном компьютере, в специальное пространство, из которого оно доступно в интернете.

Популярные - Jenkins или GitLab CI

О DevOps и его практиках

    DevOps, грубо говоря, регламентирует «завод» и «конвейер» на нём.

    Одной из практик DevOps является CI/CD/CDP. Эта практика описывает шаги «конвейера».

    CI (Continuous Integration, непрерывная интеграция) – начальная стадия «конвейера» по сборке кода и загрузке собранного ПО в среду разработки.

    CD (Continuous delivery, непрерывная поставка) – является продолжением CI. В этой практике производится автоматизированное развертывание на тестовую среду продукта и разнообразные тесты над ним.

    CDP (Continuous Deployment, непрерывное развертывание) – поставка результатов работы CI и CD практик в промышленную среду.

Jenkins или GitLab, в свою очередь, может реализовать CI/CD/CDP на практике.


### `Composer` [&uarr;](#devmap)

Composer — менеджер пакетов для PHP.

Composer, менеджер зависимостей для PHP, был выпущен около 10 лет назад, а недавно опубликовали новую версию, 2.0. За эти годы Composer получил множество новых функций и не отставал от стандартов PHP. Вторая версия совместима со старыми проектами, но привнесет еще несколько замечательных новых функций.

Список улучшений:

Ускорение и оптимизация

    В этом релизе была улучшена производительность. Теперь он не учитывает уже установленные пакеты, в результате обновление проходит намного быстрее. Также улучшили работу с памятью и процессором.

    Команда установки стала умнее. Теперь она не изменяет папку vendor до тех пор, пока не гарантирует, что все пакеты установлены. Это позволяет избежать потери времени на их удаление в случае ошибок во время процесса.

    Одной из лучших функций, для меня, стала параллельная загрузка, которая теперь поддерживается из коробки, и нам не нужно устанавливать дополнительные пакеты, как hirak/prestissimo, чтобы это работало.

Поддержка оффлайн

    Composer представил возможность использовать его в оффлайн режиме. Это может быть интересно для бенчмарков или при возникновении проблем с подключением к интернету

Поддержка --dry-run для “require” и “remove”

    Эта опция уже была доступна при обновлении пакетов (composer update --dry-run. Она позволяет нам увидеть, что произойдет при запуске команды, просто отображая данные в терминале, без реальных изменений в вашем проекте или папке vendor.

    Composer 2.* дает возможность использовать опцию с composer require и composer remove что делает нашу жизнь проще

Предотвращение проблем при работе от root

    При выполнении команд от пользователя root теперь требуется подтверждение, чтобы предотвратить наши ошибки.

Канонические репозитории

    Эта функция решает определенную проблему, с которой сталкиваются несколько разработчиков при использовании одного и того же пакета в разных репозиториях. Composer 1.0 следовал определенному порядку при установке/обновлении своих пакетов. Он начал искать пакеты в своем списке репозиториев до тех пор, пока пакет не будет найден (также последняя версия).

    Иногда это было не совсем ожидаемое поведение при работе с различными версиями пакетов в других репозиториях (используемых одним и тем же проектом). Представьте себе, что иногда вы хотите загрузить пакет из своего частного репозитория, а не из packagist, в котором есть пакет с тем же именем.

    По умолчанию, в composer 2.x, все репозитории являются каноническими. Composer 1.x рассматривал все репозитории как неканонические, и для того, чтобы поменять поведение вручную, вы можете сделать

    {
        "repositories": [
            {
                "type": "composer",
                "url": "https://example.org",
                "canonical": false
            }
        ]
    }

Игнорировать определенное требование платформы

    Если по какой-то причине вы хотите проигнорировать какое-либо определенное требование платформы, вы можете просто запустить команду: composer install --ignore-platform-req php

    Она пропустит требование для PHP или конкретной версии. Если вы собираетесь игнорировать все требования, вам следует использовать команду, уже доступную в предыдущих версиях --ignore-platform-reqs




### `docker` [&uarr;](#devmap)

Docker — это технология, которая позволяет создавать и использовать приложения в «родном» окружении. В основе Docker лежит идея: если приложение работает у вас, то оно должно работать где угодно. Способ этого добиться очень простой — нужно упаковать настройки окружения вместе с приложением.

Docker чаще всего применяется для развёртывания серверных приложений, но может использоваться и в мире фронтенда.

Docker Compose — это инструмент для запуска мультиконтейнерного приложения, которое не зависит от платформы и содержит все необходимые для работы технологии и библиотеки. Конфигурация такого приложения записывается в одном текстовом файле в формате YAML. Запускается приложение одной командой в терминале.

В файле compose.yaml могут быть следующие элементы верхнего уровня:

— version (скоро исключат): информация о версии формата файла конфигурации;
— services (обязательный): список всех контейнеров, которые нужно будет запустить;
— networks: список подсетей Docker Network, которые объединяют группы контейнеров в виртуальную локальную сеть (она может быть доступна из внешнего мира);
— volumes: список томов, которые будут доступны контейнерам, описанным в файле конфигурации;
— configs: список параметров, которые позволяют запускать контейнеры в различных режимах, не собирая их заново;
— secrets: список чувствительных с точки зрения безопасности параметров (то же, что и configs, но специального назначения).

Обычно жизненный цикл контейнера состоит из следующей последовательности состояний:

    Создание контейнера
    Работа контейнера
    Приостановка контейнера
    Возобновление работы контейнера
    Запуск контейнера
    Остановка контейнера
    Перезапуск контейнера
    Принудительная остановка контейнера
    Удаление контейнера


### Важные службы

Движок Docker Engine — приложение для управления объектами Docker. Оно включает в себя три компонента:

    -  интерфейс (Docker API);
    -  консольный клиент (Docker CLI) - в терминал вводят команды, начинающиеся с ключевого слова docker, обращаясь к клиенту. Затем клиент использует API Docker для отправки команд демону Docker.
    -  сервер (Docker Daemon) — это сервер Docker, который ожидает запросов к API Docker. Демон Docker управляет образами, контейнерами, сетями и томами.

Ваш компьютер называется Docker Host. Все операции, которые мы выполняем в интерфейсе или через консоль, выполняются сервером через API движка.

Docker Desktop — пакет приложений с графическим интерфейсом, включающий специальную виртуальную машину для работы с движком, визуальный интерфейс (Dashboard), консольный клиент, инструменты для работы с реестром Docker Hub и пр.

### Объекты Docker

    Образ (Docker Image) — прототип будущего контейнера, содержащий операционную систему, приложение или проект для сборки приложения. Образы состоят из слоёв. Каждый новый слой — это надстройка над предыдущим. Слои должны надстраиваться поверх базового образа, формируя новый. Например, базовым образом может быть образ операционной системы.

    Контейнер (Docker Container) — уже собранное и запущенное приложение в изолированном окружении, которое формируется послойно, в соответствии с образом. Каждый новый слой расширяет функциональность предыдущего, формируя стек используемых инструментов, платформ и настроек системных служб. Файловая система контейнера тоже стековая (Union File Systems). Каталоги и файлы отдельного слоя образа накладываются друг на друга, образуя единое целое.

    Том (Docker Volume) — папка, которую можно подключить (говорят «примонтировать») к контейнерам. Папка может быть связана с конкретной папкой на вашем компьютере, а может быть как бы сетевой для контейнеров на вашем компьютере. Тома необходимы для хранения файлов конфигурации, критических с точки зрения безопасности, файлов баз данных, файлов, которые нельзя удалять после окончания работы приложения.

    Сеть (Docker Network) — виртуальная локальная сеть, которая позволяет совместно использовать несколько запущенных контейнеров и соединять запущенный контейнер с вашим компьютером. В основном вы будете использовать три режима работы сетевой инфраструктуры Docker:

    bridge — когда контейнеры могут взаимодействовать между собой как веб-сервер и база данных.
    host — для доступа к локальному сетевому окружению на вашем компьютере.
    none — сеть для контейнеров полностью отключена.

### Инструменты

Docker Hub (реестр) — официальный реестр образов.

Опубликованные образы хранятся в Docker Hub. Существуют и другие публичные реестры образов:

    Google Cloud Container Registry;
    Azure Container Registry;
    IBM Cloud Container Registry;
    Oracle Cloud Infrastructure Container Registry;
    Yandex Container Registry.

Репозиторий Docker

    Репозиторием Docker (Docker Repository) называют набор образов Docker, обладающих одинаковыми именами и разными тегами. Теги — это идентификаторы образов.

    Обычно в репозиториях хранятся разные версии одних и тех же образов. Например, Python — это имя популярнейшего официального репозитория Docker на хабе Docker. А вот Python:3.7-slim — это версия образа с тегом 3.7-slim в репозитории Python. В реестр можно отправить как целый репозиторий, так и отдельный образ.

Docker CLI — консольный клиент, позволяющий управлять Docker через интерфейс командной строки.

Консольный клиент содержит команды для управления объектами Docker. Список основных команд:

    docker ps;
    docker run;
    docker image;
    docker container;
    docker volume.


### Как пользоваться

Ключи командного интерфейса Docker CLI хорошо проработаны и похожи на консольные команды в bash. Например, дополнительный ключ prune позволяет удалять неиспользуемые объекты. Ключ rm служит для удаления, а ключ ls для просмотра объектов. Объекты Docker в обязательном порядке имеют уникальное имя. Если вы не именуете объект специально, то имя объекта формируется с помощью хэш-функции. Если вы попытаетесь создать объект одного и того же типа с уже использованным именем, в этом вам будет отказано.

Мониторинг запущенных контейнеров

    docker ps — просмотр запущенных контейнеров.
    docker ps -a — ключ -a выводит и запущенные, и остановленные контейнеры.
    docker ps -s — ключ -s выводит дисковое пространство, используемое каждым запущенным контейнером.
    docker ps -f name=hello — ключ -f фильтрует список контейнеров по имени, например, hello.

### Запуск контейнеров

Для запуска контейнера, который доступен локально или на Docker Hub, выполните команду:

    docker run --name test -i -t hello 
    или
    docker exec -it тут_имя_контейнера bash

Ключ --name используется для установки имени запущенного контейнера. Ключи -i и -t указывают, что для запуска контейнера будет использоваться стандартный поток ввода и терминал TTY соответственно. Для того чтобы при запуске контейнера примонтировать том, который будет связан с папкой на вашем компьютере, а потом получить доступ к контейнеру через терминал, выполните команду:

    docker run -t -i --mount type=bind,src=/data,dst=/data hello bash

### Управление образами

Вы можете получить список всех доступных локально образов с помощью команды:

    docker image ls

Ключи prune, rm действуют обычным способом, позволяя удалить неиспользуемые или конкретные образы соответственно. Для работы с реестром необходимо использовать следующие команды:

    docker image pull hello — загрузка образа с именем hello из реестра;
    docker image push hello — отправка образа с именем hello в реестр;
    docker image inspect hello — полная информация о контейнере hello;
    docker image build — собрать контейнер из текущей папки с учётом Dockerfile.


### Периодическая чистка данных Докера

    docker container prune
    docker image prune
    docker volume prune
    docker network prune

### скачать все образы докера

    docker pull docker.ru/php/php-apache -a
    docker pull docker.ru/php/php-fpm -a

### Управление томами

    docker volume ls — вывод всех томов.
    docker volume ls -f name=hello — вывод всех томов с фильтрацией по имени, например, hello.
    docker volume create hello — создание нового тома, например, hello.
    docker volume inspect hello — исчерпывающая информация о томе.


### Образы Docker

Контейнер Docker — это образ Docker, вызванный к жизни. Это — самодостаточная операционная система, в которой имеется только самое необходимое и код приложения.

Образы Docker являются результатом процесса их сборки, а контейнеры Docker — это выполняющиеся образы. В самом сердце Docker находятся файлы Dockerfile. Подобные файлы сообщают Docker о том, как собирать образы, на основе которых создаются контейнеры - инструкции, при сборке образа, обрабатываются сверху вниз.

При запуске команды docker build для создания нового образа подразумевается, что Dockerfile находится в текущей рабочей директории.

Слои в итоговом образе создают только инструкции FROM, RUN, COPY, и ADD. Другие инструкции что-то настраивают, описывают метаданные, или сообщают Docker о том, что во время выполнения контейнера нужно что-то сделать, например — открыть какой-то порт или выполнить какую-то команду.


Дюжина инструкций Dockerfile

    FROM — задаёт базовый (родительский) образ.
    LABEL — описывает метаданные. Например — сведения о том, кто создал и поддерживает образ.
    ENV — устанавливает постоянные переменные среды.
    RUN — выполняет команду и создаёт слой образа. Используется для установки в контейнер пакетов.
    COPY — копирует в контейнер файлы и папки.
    ADD — копирует файлы и папки в контейнер, может распаковывать локальные .tar-файлы.
    CMD — описывает команду с аргументами, которую нужно выполнить когда контейнер будет запущен. Аргументы могут быть переопределены при запуске контейнера. В файле может присутствовать лишь одна инструкция CMD.
    WORKDIR — задаёт рабочую директорию для следующей инструкции.
    ARG — задаёт переменные для передачи Docker во время сборки образа.
    ENTRYPOINT — предоставляет команду с аргументами для вызова во время выполнения контейнера. Аргументы не переопределяются.
    EXPOSE — указывает на необходимость открыть порт.
    VOLUME — создаёт точку монтирования для работы с постоянным хранилищем.


Файл .dockerignore

    Вот что даёт тому, кто занимается созданием образов Docker, применение файлов .dockerignore:

    1) Это позволяет исключать из состава образа файлы, содержащие секретные сведения наподобие логинов и паролей.
    2) Это позволяет уменьшить размер образа. Чем меньше в образе файлов — тем меньше будет его размер и тем быстрее с ним можно будет работать.
    3) Это даёт возможность уменьшить число поводов для признания недействительным кэша при сборке похожих образов. Например, если при повторной сборке образа меняются некие служебные файлы проекта, наподобие файлов с журналами, из-за чего данные, хранящиеся в кэше, по сути, необоснованно признаются недействительными, это замедляет сборку образов.




Несколько советов, касающихся эффективного использования кэша Docker:

    1) Используйте всегда, когда это возможно, официальные образы в качестве базовых образов. Официальные образы регулярно обновляются, они безопаснее неофициальных образов.
    
    2) Для того чтобы собирать как можно более компактные образы, пользуйтесь базовыми образами, основанными на Alpine Linux.
    
    3) Если вы пользуетесь apt, комбинируйте в одной инструкции RUN команды apt-get update и apt-get install. Кроме того, объединяйте в одну инструкцию команды установки пакетов. Перечисляйте пакеты в алфавитном порядке на нескольких строках, разделяя список символами \. Например, это может выглядеть так:

    RUN apt-get update && apt-get install -y \
        package-one \
        package-two \
        package-three
    && rm -rf /var/lib/apt/lists/*


    Этот метод позволяет сократить число слоёв, которые должны быть добавлены в образ, и помогает поддерживать код файла в приличном виде.

    4) Включайте конструкцию вида && rm -rf /var/lib/apt/lists/* в конец инструкции RUN, используемой для установки пакетов. Это позволит очистить кэш apt и приведёт к тому, что он не будет сохраняться в слое, сформированном командой RUN.
    
    5) Разумно пользуйтесь возможностями кэширования, размещая в Dockerfile команды, вероятность изменения которых высока, ближе к концу файла.
    
    6) Пользуйтесь файлом .dockerignore.
    
    7) Взгляните на dive — отличный инструмент для исследования образов Docker, который помогает в деле уменьшения их размеров.


Создание образов

    Вот команда, которая позволяет собирать образы Docker:

    docker image build -t my_repo/my_image:my_tag .

    Флаг -t — это сокращение для --tag. Он указывает Docker на то, что создаваемому образу надо назначить предоставленный в команде тег. В данном случае это my_tag.

    Точка в конце команды указывает на то, что образ надо собрать с использованием файла Dockerfile, находящегося в текущей рабочей директории.

    После того, как образ собран, его можно отправить в удалённый реестр. Благодаря этому им смогут воспользоваться другие люди, его можно будет загрузить и запустить на другом компьютере. Предположим, вы хотите использовать Docker Hub. 

    После того, как вы зарегистрируетесь на Docker Hub, вам нужно войти в систему:

    docker login

    После входа в систему можно будет отправлять образы в реестр:

    docker image push my_repo/my_image:my_tag



### Стандарты написания кода [&uarr;](#devmap)

### Восемь общих правил

Существуют правила, которые подойдут для написания кода на любом языке программирования. Только следование им уже повысит качество вашего кода.

    Придумывайте понятные и читаемые названия. Избегайте русских слов в латинской транскрипции. Только английские слова, обозначающие суть.
    Делайте отступы на каждом уровне и отделяйте логические блоки пустой строкой.
    Сокращайте вложенность кода и убирайте дублирование.
    Контролируйте длину. Рекомендуем для функций не более 20 строк, для метода не более 50 строк, для класса не более 300 строк, для файла — не более 1000 строк. Также ограничивайте длину одной строки до видимого значения на экране. Мягкое ограничение составляет 120 символов.
    Комментируйте и документируйте код. Это позволит зафиксировать всю необходимую информацию.
    Используйте рефакторинг. Следуйте принципу «рефакторинг — раньше и рефакторинг — чаще». Советуем также прочитать книгу «Рефакторинг. Улучшение проекта существующего кода» Мартина Фаулера.
    Работайте в системе контроля версий, например, Git. Это бесплатно и удобно. Обучиться работать в ней можно за 11 занятий на видеокурсе «Git. Быстрый старт».
    Изучайте Open Source код. Вы сможете увидеть, как пишут ведущие разработчики и воспользоваться лучшими практиками в программировании

### Статический анализ кода 

Часто можно сказать, насколько код программы корректен, даже не запуская её. Процесс исследования исходного кода без запуска называют статическим анализом или линтингом, а программу, которая это делает — статическим анализатором или линтером.

Линтер — это программа, которая разбирает исходный код на стандартизированные кусочки, а потом даёт эти кусочки на проверку специальным плагинам. Плагин получает разобранный участок кода и проверяет его на корректность по ряду правил, которые определены в этом плагине.

Самый популярный линтер для JavaScript — это ESLint. Он находит участки кода, которые могут потенциально привести к ошибкам, и сообщает об этом.

Чтобы воспользоваться ESLint, нужно установить его через менеджер зависимостей в папке проекта. 

npm install --save-dev eslint

После установки нужно инициализировать конфигурационный файл .eslintrc.json. 

npx eslint --init

В созданный файл можно добавлять правила, по которым линтер будет работать. Обычно используют стандартную конфигурацию, поверх которой добавляют правила, подходящие для конкретного проекта.

{
  // ...
  "extends": "eslint:recommended",
  // ...
}

Теперь можно добавить в package.json новый скрипт, который будет запускать статический анализ.

{
  // ...
  "scripts": {
    "lint": "eslint ./**/*.js"
  }
  // ...
}

После запуска линтера командой npm run lint, в консоли появится результат его работы.

На самом деле, статически проанализировать можно почти любой язык программирования. Поэтому, при разработке веб-приложений часто не останавливаются на использовании ESLint, а добавляют ещё Stylelint — линтер для стилей. Он умеет работать не только с CSS, но и почти с любым языком описания стилей, например, CSS-in-JS, SCSS или Stylus.

Автоматический запуск

    Не хочется нагружать голову разработчика необходимостью запускать линтер самостоятельно, и это автоматизируют:

    - запускают линтер на серверах и показывают результат программисту, этот подход называют CI, он широко применяется в индустрии;
    - добавляют автоматический вызов линтера при создании нового коммита в репозитории;
    - интегрируют статический анализ в среду разработки, так можно получать рекомендации прямо во время написания кода и сразу на них реагировать.


### PSR

PSR (PHP Standards Recommendations) — описывает общие концепции, которые уже были проверены и отработаны.

Список PSR стандартов расширяется новыми, а сами стандарты делятся на категории:

    - Автозагрузка, 
    - Интерфейсы, 
    - HTTP
    - Стиль кодирования

    каждому из которых присваивается определенный статус:
    Принят, Устаревший, Черновик и Заброшенный.

Стиль кодирования:

    Цель следующих PSR стандартов уменьшить когнитивное искажение при чтении кода от разных авторов.

     - PSR-1: Basic Coding Standard
     - PSR-2 — Coding Style Guide Устарел
     - PSR-12: Extended Coding Style Guide 

Описанные выше спецификации достаточно объемные, поэтому мы рассмотрим только базовые из PSR-1:

    - Использование только тэгов <?php и <?=
    - Только UTF-8 без BOM для php кода
    - Не стоит мешать разный функционал в одном файле (1 файл = 1 класс)
    - Пространство имен и классы должны следовать [PSR-0, PSR-4]
    - Классы объявляются в `StudlyCase`
    - Константы объявляются в ТАКОМ_ВИДЕ
    - Методы объявляются в `camelCase`

На самом деле, нет смысла помнить про кажный пункт о переносе скобки, пробеле, табе и т.п., так как существует различный функционал, который позволяет автоматически проверить и отформатировать кодовую базу по стандарту PSR-2/PSR-12:

    - Ручной режим: можно использовать reformat code в phpStorm.
    - Более продвинутый вариант: использовать какой-нибуть кодснифер, например PHP CS Fixer (часто используется в CI, чтобы не принимать коммиты с неотформатированным кодом).


###  Код-ревью 

Код-ревью — это область, где особенно ярко проявляются софт-скиллы инженеров. Провести хорошее код-ревью сложнее, чем написать хороший код.

    Отправка изменений на код-ревью происходит через пул-реквесты. Для прохождения код-ревью нужно получить одобрение одного или нескольких коллег. Способ выбора коллег для проведения ревью зависит от процессов внутри компании.

    Пул-реквест (PR) — это предложение слить изменения в ветке разработчика с другой веткой. Иногда их называют мёрж-реквестами (MR).

Conventional comments

Conventional comments предлагает формат, где сообщение описывается как:

    <label> [decorations]: <subject>
    [discussion]

    label - тип комментария;
    subject - основная мысль комментария;
    decorations (опционально) - дополнительные лэйблы для комментария. Они окружены скобками;
    discussion (опционально) - здесь содержатся подтверждающие заявления, контекст, рассуждения и все остальное, чтобы помочь сообщить "почему" и "последующие шаги" для разрешения замечания;

Например:

    question (non-blocking): На этом этапе имеет значение, какой поток выиграл?
    Может быть, чтобы предотвратить состояние гонки, мы должны продолжать зацикливаться, пока все они не выиграют?


Настоятельно рекомендуется использовать следующие лэйблы:

    praise - «Похвала» подчёркивает что-то положительное. Попробуйте оставить хотя бы один такой комментарий. Не оставляйте ложных похвал (которые на самом деле могут нанести вред).

    nitpick - «Придирки» - это небольшие, но необходимые изменения. Придирчивые комментарии значительно помогают направить внимание читателя на комментарии, требующие большего внимания.

    suggestion - «Предложения» предоставляют способы по совершенствованию в определённой теме. Важно быть предельно ясным в том, что предлагается и почему именно это улучшение. Рассмотрите возможность использовать блокирующие и не блокирующие декорации для последующего информирования о ваших намерениях.

    issue - «Проблемы» высвечивают конкретные трудности рассматриваемого вопроса. Если вы не уверены, существует проблема или нет - рассмотрите возможность оставить question.

    question - «Вопросы» допустимы, если у вас есть потенциальная проблема, но не уверены уместна она или нет. Обращение к автору с просьбой о разъяснении или расследовании может привести к быстрому урегулированию этого вопроса.
    
    thought - «Мысли» представляют собой идею, которая всплыла в процессе ревью. Эти замечания по своей природе не блокируют, но они чрезвычайно ценны и могут привести к более целенаправленным предложениям и возможностям наставничества.

    chore - «Рутинная работа» - это небольшие задачи которые необходимо выполнить до того как пулреквест (или другая форма ревью) «официально» будут приняты. Обычно в таких комментариях упоминаются какие-то общие процессы. Постарайтесь оставить ссылку в описании на процесс, чтобы автор мог понять как именно выполнять рутинную работу.

Декорации дают дополнительный контекст для комментария:

    (non-blocking) - комментарий с такой декорацией не должен препятствовать принятию рассматриваемого пулреквеста. Это полезно для команд, которые рассматривают блокирование комментариев по умолчанию.

    (blocking) - комментарий с такой декорацией должен препятствовать принятию рассматриваемого вопроса до тех пор, пока он не будет решён. Это полезно для команд, которые считают, что комментарии не блокируются по умолчанию.

    (if-minor) - эта декорация даёт автору некоторую свободу действий, которая заключается в том, что он может разрешить комментарий только в том случае, если изменения окажутся незначительными или тривиальными.



### Шаги по оптимизации сайта [&uarr;](#devmap)

Бэкенд и запросы к бд:

    - анализируем лог sql запросов 
    - Убираем запросы в цикле
    - В запрашиваемых данных из БД в массиве select оставляем только нужные.
    - Если время запроса превышает 0.001 секунду - оптимизируем запрос, думаем над созданием индексов
    - Проверяем, настроено ли кэширование и параметры, от которого оно зависит.


Конфигурация и производительность сервера:

    - Версия Mysql должна быть не ниже выше 5.7, версия PHP - не ниже 7.0
    - Желательно наличие акселератора PHP (OPcache, XCache, APC и другого). Лучше всего OPcache

Фронтенд - работа со скриптами, стилями, статикой, изображениями и пр:

    Проверяем сайт через:

    - https://developers.google.com/speed/pagespeed/insights/ - оценка для мобильных устройств и декстопа должна находится в зелёной зоне
    - https://tools.pingdom.com/ - вес страницы не должен превышать 2-3 Mb
    - В консоли Chrome в панели audits показатели должны быть выше 60%

    Что должно быть реализовано:

    - Статическая информация должна кэшироваться браузером (проверить настройки в .htaccess)
    - Для загрузки картинок, не входящих в первый экран, обязательно использовать lazy-load. Это так же касается вторых, третьих и тд картинок на слайдерах
    Картинки должны быть оптимизированы для веба и отресайзены на нужные размеры
    - Использовать для картинок тэг picture
    - Минификация / обфускация js скриптов
    - Количество подгружаемых на странице js-скриптов должно быть ограничего, т.к. стандартно браузеры могут загружать файлы не более чем в 6-10 потоков с одного хоста. Т.е. работать будет быстрее, если собрать несколько мелких js в один большой, чем загружать 40 маленьких отдельно. 
    - Сторонние стандартные библиотеки лучше подгружать извне, т.к. существует вероятность, что браузер пользователя уже их выкачал и закэшировал


# Разработка

### Интересный крон (запуск скрипта раз в 30 секунд) [&uarr;](#devmap)

Крон это минимум минута.
Раз в 30 секунд это значит запускать каждую минуту скрипт который что-то делает, потом спит 30 секунд и еще раз делает.
Как вариант, написать скрипт, который будет запускаться раз в минуту и дёргать файл дважды, между двумя дёрганьями спать 30 секунд.

### Защита от спама - предложить интересный способ [&uarr;](#devmap)

Google ReCaptcha
Блэк лист
Hidden поле которое потом сможем проверять на бэке

### Защита от повторной отправки форм [&uarr;](#devmap)

Ограничение частоты отправки сообщений
Ограничение количества сообщний от одного пользователя
ограничение количества отправляемых сообщений за определенный отрезок времени

### Сравнение алгоритмов сортировки [&uarr;](#devmap)

Сортировка массивов — самая базовая алгоритмическая задача, которую нередко спрашивают на собеседованиях. С другой стороны, в реальном коде массивы сортируют, используя уже готовые функции стандартной библиотеки. Тогда для чего задают подобные вопросы? Обычно собеседующий хочет узнать следующее:

1) Насколько вы вообще в курсе о существовании алгоритмов.
2) Способны ли вы программировать.
3) Как работает ваше алгоритмическое мышление.


    Selection sort (сортировка выбором) – суть алгоритма заключается в проходе по массиву от начала до конца в поиске минимального элемента массива и перемещении его в начало. Сложность такого алгоритма O(n2).

    Bubble sort (сортировка пузырьком) – данный алгоритм меняет местами два соседних элемента, если первый элемент массива больше второго. Так происходит до тех пор, пока алгоритм не обменяет местами все неотсортированные элементы. Сложность данного алгоритма сортировки равна O(n^2).

    Insertion sort (сортировка вставками) – алгоритм сортирует массив по мере прохождения по его элементам. На каждой итерации берется элемент и сравнивается с каждым элементом в уже отсортированной части массива, таким образом находя «свое место», после чего элемент вставляется на свою позицию. Так происходит до тех пор, пока алгоритм не пройдет по всему массиву. На выходе получим отсортированный массив. Сложность данного алгоритма равна O(n^2).

    Quick sort (быстрая сортировка) – суть алгоритма заключается в разделении массива на два под-массива, средней линией считается элемент, который находится в самом центре массива. В ходе работы алгоритма элементы, меньшие чем средний будут перемещены в лево, а большие в право. Такое же действие будет происходить рекурсивно и с под-массива, они будут разделяться на еще два под-массива до тех пор, пока не будет чего разделать (останется один элемент). На выходе получим отсортированный массив. Сложность алгоритма зависит от входных данных и в лучшем случае будет равняться O(n×2log2n). В худшем случае O(n^2). Существует также среднее значение, это O(n×log2n).

    Comb sort (сортировка расческой) – идея работы алгоритма крайне похожа на сортировку обменом, но главным отличием является то, что сравниваются не два соседних элемента, а элементы на промежутке, к примеру, в пять элементов. Это обеспечивает от избавления мелких значений в конце, что способствует ускорению сортировки в крупных массивах. Первая итерация совершается с шагом, рассчитанным по формуле (размер массива)/(фактор уменьшения), где фактор уменьшения равен приблизительно 1,247330950103979, или округлено до 1,3. Вторая и последующие итерации будут проходить с шагом (текущий шаг)/(фактор уменьшения) и будут происходить до тех пор, пока шаг не будет равен единице. Практически в любом случае сложность алгоритма равняется O(n×log2n).

Для сортировки неотсортированного массива, наиболее оптимальным из представленных алгоритмов для сортировки массива является быстрая сортировка. Несмотря на более длительное время выполнения алгоритм потребляет меньше памяти, что может быть важным в крупных проектах. 

Однако такие алгоритмы как сортировка выбором, обменом и вставками могут лучше подойти для научных целей, например, в обучении, где не нужно обрабатывать огромное количество данных. 

При частично отсортированном массиве результаты не сильно отличаются, все алгоритмы сортировки показывают время примерно на 2-3 миллисекунды меньше. Однако при сортировке частично отсортированного массива быстрая сортировка срабатывает намного быстрее и потребляет меньшее количество памяти.

Пример пузырьковой сортировки

    <?php

    function bubbleSort($coll)
    {
        $size = count($coll);
        // do..while цикл. Работает почти идентично while
        // Разница в проверке. Тут она делается не до выполнения тела, а после.
        // Такой цикл полезен там, где надо выполнить тело хотя бы раз в любом случае.
        do {
            // Объявляем переменную swapped, значение которой показывает был ли
            // совершен обмен элементов во время перебора массива
            $swapped = false;
            // Перебираем массив и меняем местами элементы, если предыдущий
            // больше, чем следующий
            for ($i = 0; $i < $size - 1; $i++) {
                if ($coll[$i] > $coll[$i + 1]) {
                    // temp – временная переменная для хранения текущего элемента
                    $temp = $coll[$i];
                    $coll[$i] = $coll[$i + 1];
                    $coll[$i + 1] = $temp;
                    // Если сработал if и была совершена перестановка,
                    // присваиваем swapped значение true
                    $swapped = true;
                }
            }
            // Уменьшаем счетчик на 1, т.к. самый большой элемент уже находится
            // в конце массива
            $size--;
        } while ($swapped); // продолжаем, пока swapped === true

        return $coll;
    }

    print_r(bubbleSort([3, 2, 10, -2, 0]));
    // => Array
    // => (
    // =>     [0] => -2
    // =>     [1] => 0
    // =>     [2] => 2
    // =>     [3] => 3
    // =>     [4] => 10
    // => )


### Веб-сервисы, отличия, когда что использовать (`soap`, архитектура `rest`) [&uarr;](#devmap)

### REST

REST означает REpresentational State Transfer (Википедия: «передача состояния представления»). Это популярный архитектурный подход для создания API в современном мире.

Протокол HTTP

    Когда вы вводите в браузере URL-адрес, например www.google.com, на сервер отправляется запрос на веб-сайт, идентифицированный URL-адресом.
    Затем этот сервер формирует и выдает ответ. Важным является формат этих запросов и ответов. Эти форматы определяются протоколом HTTP — Hyper Text Transfer Protocol.

    Когда вы набираете URL в браузере, он отправляет запрос GET на указанный сервер. Затем сервер отвечает HTTP-ответом, который содержит данные в формате HTML — Hyper Text Markup Language. Затем браузер получает этот HTML-код и отображает его на экране.

    Допустим, вы заполняете форму, присутствующую на веб-странице, со списком элементов. В таком случае, когда вы нажимаете кнопку «Submit» (Отправить), HTTP-запрос POST отправляется на сервер.

HTTP и RESTful веб-сервисы

    HTTP обеспечивает базовый уровень для создания веб-сервисов. Поэтому важно понимать HTTP. Вот несколько ключевых абстракций.

    Ресурс

    Ресурс — это ключевая абстракция, на которой концентрируется протокол HTTP. Ресурс — это все, что вы хотите показать внешнему миру через ваше приложение. Например, если мы пишем приложение для управления задачами, экземпляры ресурсов будут следующие:

        Конкретный пользователь
        Конкретная задача
        Список задач


    URI ресурса

    Когда вы разрабатываете RESTful сервисы, вы должны сосредоточить свое внимание на ресурсах приложения. Способ, которым мы идентифицируем ресурс для предоставления, состоит в том, чтобы назначить ему URI — универсальный идентификатор ресурса. Например:

        Создать пользователя: POST /users
        Удалить пользователя: DELETE /users/1
        Получить всех пользователей: GET /users
        Получить одного пользователя: GET /users/1

REST и Ресурсы 

    Важно отметить, что с REST вам нужно думать о приложении с точки зрения ресурсов:
    Определите, какие ресурсы вы хотите открыть для внешнего мира
    Используйте глаголы, уже определенные протоколом HTTP, для выполнения операций с этими ресурсами.

    Вот как обычно реализуется служба REST:

    - Формат обмена данными: здесь нет никаких ограничений. JSON — очень популярный формат, хотя можно использовать и другие, такие как XML
    
    - Транспорт: всегда HTTP. REST полностью построен на основе HTTP.
    
    - Определение сервиса: не существует стандарта для этого, а REST является гибким. Это может быть недостатком в некоторых сценариях, поскольку потребляющему приложению может быть необходимо понимать форматы запросов и ответов. Однако широко используются такие языки определения веб-приложений, как WADL (Web Application Definition Language) и Swagger.


    REST фокусируется на ресурсах и на том, насколько эффективно вы выполняете операции с ними, используя HTTP.

Компоненты HTTP

    HTTP определяет следующую структуру запроса:

        строка запроса (request line) — определяет тип сообщения
        заголовки запроса (header fields) — характеризуют тело сообщения, параметры передачи и прочие сведения
        тело сообщения (body) — необязательное


    HTTP определяет следующую структуру ответного сообщения (response):

        строка состояния (status line), включающая код состояния и сообщение о причине
        поля заголовка ответа (header fields)
        дополнительное тело сообщения (body)


    Метод, используемый в HTTP-запросе, указывает, какое действие вы хотите выполнить с этим запросом. Важные примеры:

        GET: получить подробную информацию о ресурсе
        POST: создать новый ресурс
        PUT: обновить существующий ресурс
        DELETE: Удалить ресурс


    Код состояния всегда присутствует в ответе HTTP. Типичные примеры:

        200 — успех
        404 — cтраница не найдена


### REST и SOAP

REST и SOAP на самом деле не сопоставимы. SOAP — это формат протокола, основанный на XML, тогда как REST — это архитектурный подход.

Давайте сравним популярные реализации стилей REST и SOAP.

    - Пример реализации RESTful: JSON через HTTP
    - Пример реализации SOAP: XML поверх SOAP через HTTP

Формат обмена сообщениями

    - В SOAP вы используете формат SOAP XML для запросов и ответов.

    - В REST такого фиксированного формата нет. Вы можете обмениваться сообщениями на основе XML, JSON или любого другого удобного формата. JSON является самым популярным среди используемых форматов.

Определения услуг

    - SOAP использует WSDL (Web Services Description Language) — язык описания веб-сервисов и доступа к ним, основанный на языке XML.
    
    - REST не имеет стандартного языка определения сервиса. Несмотря на то, что WADL был одним из первых предложенных стандартов, он не очень популярен. Более популярно использование Swagger или Open API.

Транспорт

    SOAP не накладывает никаких ограничений на тип транспортного протокола. Вы можете использовать либо Web протокол HTTP, либо MQ.

    REST подразумевает наилучшее использование транспортного протокола HTTP

Простота реализации

    RESTFful веб-сервисы, как правило, гораздо проще реализовать, чем веб-сервисы на основе SOAP.

    REST обычно использует JSON, который легче анализировать и обрабатывать. В дополнение к этому, REST не требует наличия определения службы для предоставления веб-службы.

    Однако в случае SOAP вам необходимо определить свой сервис с использованием WSDL, и при обработке и анализе сообщений SOAP-XML возникают большие накладные расходы.

Всякий раз, когда вы разрабатываете сервис, такой как REST API или SOAP API, вы можете выбрать один из двух подходов:

    - Code First и генерируйте контракт из кода
    - Contract First и разработка кода на основе контракта

### HATEOAS

Термин HATEOAS означает фразу «Hypermedia As The Engine Of Application State» (Гипермедиа как двигатель состояния приложения). 


Когда вы отправляете запрос для получения данных к примеру -  учетной записи, вы получаете оба:

    Номер счета и данные баланса
    Ссылки, которые обеспечивают действия, чтобы сделать депозит/снятие/перевод/закрытие


С HATEOAS запрос на REST ресурс дает мне как данные, так и действия, связанные с данными.

Зачем нам нужен HATEOAS?

Единственная самая важная причина для HATEOAS — слабая связь (loose coupling). Если потребителю службы REST необходимо жестко закодировать все URL-адреса ресурсов, он тесно связан с реализацией вашей службы. Вместо этого, если вы вернете URL-адреса, которые он может использовать для действий, он будет слабосвязанным. Нет строгой зависимости от структуры URI, так как она указана и используется в ответе. 

### HAL — язык гипертекстовых приложений 

При разработке службы RESTful необходимо указать, как возвращать данные и ссылки, соответствующие запросу. HAL — это формат, который обеспечивает простой и согласованный способ гиперссылки между ресурсами в вашем REST API.


### `raise condition` [&uarr;](#devmap)

Состояние гонки (англ. race condition), также конкуренция[1] — ошибка проектирования многопоточной системы или приложения, при которой работа системы или приложения зависит от того, в каком порядке выполняются части кода. 

Термин состояние гонки относится к инженерному жаргону и появился вследствие неаккуратного дословного перевода английского эквивалента. В более строгой академической среде принято использовать термин неопределённость параллелизма. 

Состояние гонки — «плавающая» ошибка (гейзенбаг), проявляющаяся в случайные моменты времени и «пропадающая» при попытке её локализовать. 

Возможные последствия

    утечки памяти
    ошибки сегментирования
    порча данных
    уязвимости
    взаимные блокировки
    утечки других ресурсов, например файловых дескрипторов

Как с этим бороться:

В первую очередь это конечно же архитектурная проблема, если правильно спроектировать веб-приложение, можно избежать подобных гонок.

Обычно, применяют следующие методы борьбы с атакой:

    Используют блокировки.

    Операция блокирует в СУБД обращения к заблокированному объекту, пока его не разблокируют. Другие стоят и ждут в сторонке. Необходимо правильно работать с блокировками, не блокировать ничего лишнего.


    Рулят изоляциями транзакций.

    Упорядоченные транзакции (serializable) — гарантируют, что транзакции будут выполнены строго последовательно, однако, это может сказаться на производительности.


    Используют мьютексные семафоры (хе-хе).

    Берут какую-нибудь штуку (например etcd). В момент вызова функций создают запись с ключом, если не получилось создать запись, значит она уже есть и тогда запрос прерывается. По окончании обработки запроса запись удаляется.


### `CORS` [&uarr;](#devmap)

Ошибка в консоли вашего браузера

    No ‘Access-Control-Allow-Origin’ header is present on the requested resource.

    Cross-Origin Request Blocked: The Same Origin Policy disallows reading the remote resource at https://example.com/

    Access to fetch at ‘https://example.com’ from origin ‘http://localhost:3000’ has been blocked by CORS policy.

Это примеры CORS-ошибок.CORS — это невероятно полезный механизм в мире неправильно настроенных веб серверов, злоумышленников, орудующих в интернете и организаций, продвигающих веб-стандарты.

Если браузер запрашивает тег из источника, который отличается от получателя по схеме, в полностью определенному имени хоста или порту, то это и есть запрос между различными источниками (cross-origin request).


Источники & cross-origin

Источник идентифицируется следующей тройкой параметров: схема, полностью определенное имя хоста и порт. Например, <http://example.com> и <https://example.com> имеют разные источники: первый использует схему http, а второй https. Вдобавок, портом для http по умолчанию является 80, тогда как для https — 443. Следовательно, в данном примере 2 источника отличаются схемой и портом, тогда как хост один и тот же (example.com).

Таким образом, если хотя бы один из трех элементов у двух ресурсов отличается, то источник ресурсов также считается разным.

CORS — это механизм, который дает контролировать доступ к тегам на веб странице по сети. Механизм классифицируется на три разные категории доступа тегов:

    - Запись из разных источников
    - Вставка из разных источников 
    - Считывание из разных источников 


Вставки из разных источников — это теги, загружаемые через <script>, <link>, <img>, <video>, <audio>, <object>, <embed>, <iframe> и т.п. Все они разрешены по умолчанию. <iframe> выделяется на их фоне, так как он используется для загрузки другой страницы внутри фрейма. Его обрамление в зависимости от источника может регулироваться посредством использования заголовка  X-Frame-options.

Что касается <img> и других вставных тегов, то они устроены так, что сами инициируют запросы из разных источников cross-origin запроса. Именно поэтому в CORS существует различие между вставкой из разных источников и считыванием из разных источников. 

Считывание из разных источников  — это теги, загружаемые через вызовы AJAX/ fetch. Все они по умолчанию заблокированы вашим браузером. Существует обходной путь для вставки таких тегов на странице, но такие трюки регулируются другой политикой, которая соблюдается в современных браузерах. 


### `SPINX` [&uarr;](#devmap)

Sphinx – система полнотекстового поиска. Из плюсов – наличие лемматизаторов ru, en, du, большое количество стеммеров индексации и поиска: full-text, фасетный, geo. Нет ничего лишнего, лишь поисковый движок с быстрой индексацией и своим индексером. При правильной конфигурации RT индексов можно достичь real time indexing. В отличие от Elasticsearch наблюдаем умеренное использование памяти.

Что касается минусов, то главный из них – необходимость самому просчитывать всю структуру индексов, а значит, масштабирование проходит не так просто. Также у системы скудный API, нет возможности для визуализации, отсутствует fuzzy-поиск по дефолту (но можно реализовать свой) и скудное community.

Sphinx написан на C++. Им пользуются такие мастодонты, как Habr, Викимапия, Craigslist, поддерживается 1С-Битрикс.

Sphinx с 2016 года прекратился свое развитие и теперь альтернатива это форк сфинкса - Manticore

### `Elasticsearch` [&uarr;](#devmap)

Теперь несколько слов про Elasticsearch. Поисковая система проводит всю индексацию внутри себя, при этом управление индексами через RESTful API. Плюсы ElasticSearch – это фасетный поиск, легкое и мощное API, большое количество стеммеров, гибкая структура индексов и создание индексов постфактум. С техниками поиска тут тоже все хорошо: Full-text search, а также алгоритмы Geo и Fuzzy.

В Elasticsearch тоже много готовых реализованных модулей для ES, есть возможность хранить данные, Real Time индексация, достаточно легко масштабируется, ETL-механизмы, и что немаловажно – обширное community. Elasticsearch написан на Java и используется в Wikimedia, Mozilla, SoundCloud, GitHub и других площадках.

Что касается минусов Elasticsearch, то в их числе отсутствие своего индексера (необходимо реализовать свой или logstash в ELK), съедает много памяти, а лемматизаторы русского текста ставятся отдельно плагином.

Если бы нас попросили описать Sphinx и Elasticsearch одним выражением, то первую систему поиска мы бы охарактеризовали словами «быстрая индексация», а вторую – «мощный API».

### гексагональная архитектура [&uarr;](#devmap)

Гексагональная архитектура, это слоеная архитектура, также иногда называемая архитектурой портов и адаптеров. Называют ее так потому, что в рамках этой архитектуры имеется концепция различных портов, которые могут быть использованы (адаптированы) для использования с другими слоями.

Основное назначение этой архитектуры:

    Позволяет взаимодействовать с приложением как пользователю, так и программам, автоматическим тестам, скриптам пакетной обработки. Также позволяет разрабатывать и тестировать приложение без каких-либо дополнительных устройств или баз данных.

Почему Гексагон

    Несмотря на то, что архитектура называется гексагональной, что должно указывать на фигуру с определенным количеством граней, основной мыслью все же является то, что граней у этой фигуры много. Каждая грань представляет собой «порт» доступа к нашему приложению или же его связь с внешним миром.

    Порт может быть представлен как какой-либо проводник входящих запросов (или данных) к нашему приложению. Например, через HTTP-порт (запросы браузера, API) приходят HTTP запросы, которые конвертируются в команды для приложения. Похожим образом могут действовать различные менеджеры очередей или что угодно взаимодействующее с приложением по протоколу пересылки сообщений (например AMQP). Все это является лишь портами через которые мы можем попросить приложение сделать какие-то действия. Эти грани составляют множество «входящих портов». Другие порты могут быть использованы для доступа к данным со стороны приложения, например порт базы данных.


Основа данной архитектуры - порты и адаптеры.

    Порты - это интерфейсы нашего приложения,

    Адаптеры -  реализация наших портов.

    Гексагон - фигура, имеющая 6 сторон, шестиугольник. В нашем случае слоистая или многогранная архитектура.

Преимущества данного метода:

    Независимость: возможность не зацикливаться на бизнес логике.
    Можно задекларировать, описать схему работы нашего приложения до создания внешних сервисов, использовать замоканные данные в реализации адаптеров.

    Гибкость: использование любых фреймворков, перенос доменов адаптеров в другие проекты, добавление новых адаптеров без изменения исходного кода.

    Легкая изменчивость: изменения в одной области нашего приложения не влияют на другие области.

Минусы

    Погружение: многим разработчикам может быть сложно освоиться, особенно, при невысоком уровне знаний. Долгое время я сам отторгал данный подход, ссылаясь на его избыточность, пока не освоил систему на практике в течение нескольких месяцев.

    Также могут возникнуть сложности реализации с graphql.


Слои приложения

    Слой предметной области (Domain Layer)

    В самом центре нашего приложения расположен слой предметной области. Этот слой содержит в себе реализацию бизнес логики и определяет, как внешние слои могут с ней взаимодействовать.
    В добавок к бизнес логике (ядро предметной области или core domain), в слое предметной области часто можно встретить дополнительную логику, например события предметной области (domain events, события, которые выбрасываются в ключевых точках бизнес логики) и “сценарии использования” или use-cases (определение того, что должно делать наше приложение).


    Слой приложения (Application Layer)

    Этот слой занимается исключительно оркестрацией действий, производимых над сущностями из слоя предметной области. Также этот слой является адаптером запросов из слоя фреймворка и отделяет его от слоя предметной области.

    Например этот слой может содержать класс-обработчик, который выполняет какой-то юз-кейс. Этот класс-обработчик принимает входящие данные, пришедшие к нему из слоя фреймворка, и будет выполнять над ними какие-то действия, которые требуются для выполнения нашего юз-кейса.

    Также он может отправлять на обработку события (domain events), которые произошли в слое предметной области.

    Это внешний слой кода, составляющего наше приложение.


    Слой фреймворка (Framework Layer)

    Наше приложение укутано в слой фреймворка (его также называют инфраструктурным слоем, infrastructure layer). Как уже было сказано выше, этот слой содержит код, который использует ваше приложение, но при этом он не является сам по себе частью приложения. Обычно этот слой представлен вашим фреймворком, но конечно же может включать в себя любые сторонние библиотеки, SDK и любой другой код. Вспомните все библиотеки, которые вы подключили через composer (предположим что мы все же пишем на PHP). Они не являются частью вашего фреймворка, но все же они объединены в один слой. Весь этот слой необходим лишь для одного — выполнять различные задачи для удовлетворения потребностей нашего приложения.


Взаимодействие слоев: границы

    Как уже было сказано выше, каждый слой регламентирует то, каким образом другим слоям можно с ним взаимодействовать. Если конкретнее, то каждый слой ответственен за определение того, каким образом с ним будет взаимодействовать каждый следующий внешний слой.

    Инструментом для этого нам послужат интерфейсы. На границе каждого слоя мы найдем интерфейсы. Эти интерфейсы являются портами для следующих слоев, в которых будут реализованы адаптеры.


SOLID как основа гексагональной архитектуры

    Строгое следование всем этим принципам приведёт к тому, что вместо интерфейсов бизнес-логики и репозиториев будет множество отдельных функциональных интерфейсов. Что же до классов, реализующих эти интерфейсы, а так же зависящих от них, то тут возможны варианты.

    Вариант, который приглянулся мне больше всего — придерживаться той же логики, один класс — реализация одного интерфейса. Каждый такой класс будет содержать свой минимум зависимостей и кода, ровно, как и его тестовый класс. Кроме того такая атомизация кода позволяет из одной кодовой базы получить абсолютно разные сервисы:

    Можно собрать модульный монолит, который предоставляет полный набор API проекта.
    
    Можно собрать набор микросервисов, каждый из которых предоставляет API своей предметной области, если их в проекте несколько.

    Можно собрать отдельные микросевисы для API запросов и API команд, применив архитектурный шаблон проектирования CQRS.

    В конечном итоге можно собрать для каждого метода API отдельный микросервис, получив в итоге FaaS.

Таким образом гексагональная архитектура может быть идеальной основой для проектов с микросервисной архитектурой.

Альтернативный вариант — реализовывать несколько взаимосвязанных интерфейсов в одном классе, но помнить, что при необходимости этот класс можно разделить.


### `KISS, DRY, YAGNI, BDUF, SOLID, APO и бритва Оккама` [&uarr;](#devmap)

1. YAGNI
You Aren’t Gonna Need It / Вам это не понадобится

Этот принцип прост и очевиден, но ему далеко не все следуют. Если пишете код, то будьте уверены, что он вам понадобится. Не пишите код, если думаете, что он пригодится позже.

Этот принцип применим при рефакторинге. Если вы занимаетесь рефакторингом метода, класса или файла, не бойтесь удалять лишние методы. Даже если раньше они были полезны – теперь они не нужны.

Может наступить день, когда они снова понадобятся – тогда вы сможете воспользоваться git-репозиторием, чтобы воскресить их из мертвых.


2. DRY
Don’t Repeat Yourself / Не повторяйтесь

Эта концепция была впервые сформулирована в книге Энди Ханта и Дэйва Томаса «Программист-прагматик: путь от подмастерья к мастеру».

Идея вращается вокруг единого источника правды (single source of truth — SSOT). Что это вообще такое?

Использование SSOT позволит создать более прочную и понятную кодовую базу.

Дублирование кода – пустая трата времени и ресурсов. Вам придется поддерживать одну и ту же логику и тестировать код сразу в двух местах, причем если вы измените код в одном месте, его нужно будет изменить и в другом.

В большинстве случаев дублирование кода происходит из-за незнания системы. Прежде чем что-либо писать, проявите прагматизм: осмотритесь. Возможно, эта функция где-то реализована. Возможно, эта бизнес-логика существует в другом месте. Повторное использование кода – всегда разумное решение.


3. KISS
Keep It Simple, Stupid / Будь проще

Этот принцип был разработан ВМС США в 1960 году. Этот принцип гласит, что простые системы будут работать лучше и надежнее.

У этого принципа много общего с переизобретением колеса, которым занимались в 1970-х. Тогда он звучал как деловая и рекламная метафора.

Применительно к разработке ПО он значит следующее – не придумывайте к задаче более сложного решения, чем ей требуется.

Иногда самое разумное решение оказывается и самым простым. Написание производительного, эффективного и простого кода – это прекрасно.

Одна из самых распространенных ошибок нашего времени – использование новых инструментов исключительно из-за того, что они блестят. Разработчиков следует мотивировать использовать новейшие технологии не потому, что они новые, а потому что они подходят для работы.


4. Big Design Up Front
Глобальное проектирование прежде всего

Этот подход к разработке программного обеспечения очень важен, и его часто игнорируют. Прежде чем переходить к реализации, убедитесь, что все хорошо продумано.

Многие разработчики считают, что если они не пишут код, то они не добиваются прогресса. Это неверный подход. Составив план, вы избавите себя от необходимости раз за разом начинать с нуля.

Иногда в недостатках и процессах разработки архитектуры должны быть замешаны и другие люди. Чем раньше вы все это обсудите, тем лучше будет для всех.

Очень распространенный контраргумент заключается в том, что стоимость решения проблем зачастую ниже стоимости времени планирования. Чем с меньшим количеством ошибок столкнется пользователь, тем лучше будет его опыт. У вас может не быть другого шанса справиться с этими ошибками.


5. SOLID

SOLID - это принципы разработки программного обеспечения, следуя которым Вы получите хороший код, который в дальнейшем будет хорошо масштабироваться и поддерживаться в рабочем состоянии.

    S - Single Responsibility Principle - принцип единственной ответственности. Каждый класс должен иметь только одну зону ответственности.

    O - Open closed Principle - принцип открытости-закрытости. Классы должны быть открыты для расширения, но закрыты для изменения.

    L - Liskov substitution Principle - принцип подстановки Барбары Лисков. Должна быть возможность вместо базового (родительского) типа (класса) подставить любой его подтип (класс-наследник), при этом работа программы не должна измениться.

    I -  Interface Segregation Principle - принцип разделения интерфейсов. Данный принцип обозначает, что не нужно заставлять клиента (класс) реализовывать интерфейс, который не имеет к нему отношения.

    D - Dependency Inversion Principle - принцип инверсии зависимостей. Модули верхнего уровня не должны зависеть от модулей нижнего уровня. И те, и другие должны зависеть от абстракции. Абстракции не должны зависеть от деталей. Детали должны зависеть от абстракций.


6. Avoid Premature Optimization
Избегайте преждевременной оптимизации

Эта практика побуждает разработчиков оптимизировать код до того, как необходимость этой оптимизации будет доказана. Думаю, что если вы следуете KISS или YAGNI, вы не попадетесь на этот крючок.

Поймите правильно, предвидеть, что произойдет что-то плохое – это хорошо. Но прежде чем вы погрузитесь в детали реализации, убедитесь, что эти оптимизации действительно полезны.

Очень простой пример – масштабирование. Вы не станете покупать 40 серверов из предположения, что ваше новое приложение станет очень популярным. Вы будете добавлять серверы по мере необходимости.

Преждевременная оптимизация может привести к задержкам в коде и, следовательно, увеличит затраты времени на вывод функций на рынок.

Многие считают преждевременную оптимизацию корнем всех зол.


7. Бритва Оккама

Что это значит в мире программирования? Не создавайте ненужных сущностей без необходимости. Будьте прагматичны — подумайте, нужны ли они, поскольку они могут в конечном итоге усложнить вашу кодовую базу.

### `mapping` в `Doctrine` [&uarr;](#devmap)

Doctrine — объектно-реляционный проектор (ORM) для PHP 7.1+, который базируется на слое абстракции доступа к БД (DBAL). Одной из ключевых возможностей Doctrine является запись запросов к БД на собственном объектно-ориентированном диалекте SQL, называемом DQL (Doctrine Query Language) и базирующемся на идеях HQL (Hibernate Query Language). 

Doctrine ORM применяется во фреймворках Laravel, Yii, Symfony. 

Doctrine ORM предоставляет разработчику удобные средства выборки данных. Это и мощный DQL для работы в объектно-ориентированном ключе, и удобный Query Builder, простой и понятный в использовании. Они покрывают большую часть потребностей, но иногда возникает необходимость использовать SQL запросы, оптимизированные или специфичные для конкретной СУБД. Для работы с результатами запросов в коде важно понимание того, как работает маппинг в Doctrine.

В основе Doctrine ORM лежит паттерн Data Mapper, изолирующий реляционное представление от объектного, и конвертирующий данные между ними. Одним из ключевых компонентов этого процесса является объект ResultSetMapping, с помощью которого описывается, как именно преобразовывать результаты запроса из реляционной модели в объектную. Doctrine всегда использует ResultSetMapping для представления результатов запроса, но обычно этот объект создается на основе аннотаций или yaml, xml конфигов, остается скрыт от глаз разработчика, потому о его возможностях знают далеко не все.

В Doctrine есть два QueryBuilder — SQL и ORM (DQL). ResultSetMapping предназначен для работы с SQL-запросами, а метод createQueryBuilder EntityManager'a вернет Вам ORM QueryBuilder. В последнем случае я не вижу необходимости использовать ручной ResultSetMapping, т.к. запрос Вы составляете уже в терминах описанных сущностей, используя DQL. В первом же случае, при использовании SQL QueryBuilder, поля в выборке переименовываться не будут.

# Повышение квалификации

### С какими `CMS`, фреймворками приходилось работать [&uarr;](#devmap)

    - внутренняя cms компании
    - bitrix скорее в качестве фреймворка чем в качестве cms
    - yii2 немного - для мелких личных проектов либо в рамках поддержки мелких сайтов
    - zend framework - немного, в рамках поддержки проекта
    - laravel - для личного проекта,фриланса и рабочих проектов

### Что больше всего нравится/не нравится в работе [&uarr;](#devmap)

Вы наверняка знаете, что не следует ругать свою нынешнюю компанию или начальника. 

Так что же делать, слыша такой вопрос? 

Самое время использовать классический подход «проблема на моей стороне» и сосредоточиться на том, почему нынешнее место работы вам не подходит. Расскажите им о своих самых сильных навыках или о проектах, которые вам больше всего нравились, но над которыми вы не смогли поработать. Скажите, что вы ищете работу, которая позволит вам чаще использовать эти навыки. Каким бы ни был ваш ответ, он должен касаться тех моментов, которые новая работа решит для вас — но также и того, какие проблемы работодателя сможете решить вы.

Вопрос, который поможет определить ориентацию соискателя на процесс или результат.

1) Люди результата

    Есть люди, которые больше ориентированы на результат. Они задаются вопросом зачем? Что получится в итоге? Это хорошие продажники, они хорошо продают и дожимают клиента. Если такой человек работает на производстве или в строительстве, он стремится сдать работу в срок. Если это руководитель, то он всегда думает, какому результату придет его подразделение и команда.

    Люди результата стремятся в кратчайший срок завершить любой начатый процесс. То есть люди результата склонны добиваться результата, но могут игнорировать технологии. Слишком сильная ориентация на результат может приводить к недоработке деталей. Такие люди обычно не очень любят писать отчеты, вести статистику, анализировать.

2) Люди процесса

    И есть люди больше ориентированные на процесс. Это хорошие бухгалтера, делопроизводители, секретари. Люди процесса технологичны и скурпулезны, но могут забывать, зачем все это делается. Для людей процесса важно все.  Для них время значения не имеет и не существует мелочей. Они плохо умеют выделять главное, поэтому по срокам выполнение задач у данной категории сотрудников обычно затягивается.

3) Есть люди с промежуточной ориентацией. Для них одинаково важно и то и другое.


Для чего все это?

    Для человека результата очень важна мотивация за достижение результата, привязка оплаты и нематериальной мотивации к результату.

    Для человека процесса более значим сам процесс, его содержание.

    И что крайне важно, людей результата важно мотивировать не только за результат, но и за соблюдение технологий.

    Людей процесса важно мотивировать, поощрять за соблюдение сроков, за соблюдение ключевых показателей результативности.

Как интерпретировать ответы?

    Если кандидат отвечает, используя совершенный вид глагола, это результат

        - Мне больше всего нравится убедить клиента
        - В работе бухгалтера больше всего нравится сданный баланс

    Если он использует слова, обозначающие процессы, глаголы несовершенного вида, это процесс

        - мне нравится убеждать, планировать, договариваться, составлять
        - анализ документов, общение с клиентами

### Почему вообще программируешь и что тебя драйвит? [&uarr;](#devmap)

Работаю программистом уже почти 6 лет. За это время успел поработать с большим количеством проектов. Нравится? Нравится. Иногда не нравится. Но я  осознанно устраивался программистом, учился на курсах,занимался самообразованием.
Иногда то, что нравится перестает нравиться, поэтому просто расскажу некоторые особенности своей работы:

    - Мне в целом близка эта тема, работать за компьютером. 
    - Каждый день узнаю что-то новое. Еще масса всего, чего я не знаю. Это круто. Подстегивает постоянно учиться, развиваться, изучать новое.
    - Общение с людьми. Раньше думал, типа ну пишешь код, ни с кем не общаешься (я по натуре экстраверт). Но нет, общаться надо и достаточно много. Иначе есть риск сделать фигню.
    - Чтобы расти надо задавать вопросы - "а зачем это нужно? А есть ли другое решение? Можно ли сделать сейчас просто, А потом исправить? А не делаю ли я фигню?". Есть программисты, которые не задают вопросы, а просто фигачат.
    - Возможность поработать с разными людьми из разной сферы. От медицины и бухгалтерии, до каких-нибудь спутников и роботов. Программирование нужно везде.
    - Возможность работать удаленно. Правда здесь зависит от компании. У удаленки есть своя специфика и нужно быть дисциплинированным. Кому-то больше заходит офис.
    - Иногда натурально мозги кипят, стресс и все такое. Валишься на диван и ничего не хочется делать. 
    - Проекты с легаси кодом - от них никуда не деться, но проблема этих проектов в том что заказчик не хочет либо не имеет средств на поддержание этого проекта в актуальном состоянии.
    - Сидячая жизнь ничего хорошего не сулит :)
    - Программист это творческая профессия - но иногда это настолько далеко от правды насколько это возможно. Большинство задач - примитивные, решающиеся лишь двумя способами - эффективным и неправильным. 

По поиску работы - хороший специалист быстро находит работу. Это чувствуется в человеке. Есть те, кто долго ищут. Обычно из-за софт-скиллов, общения, например. Людей ищут не только по навыкам, но и тому, как он мыслит, как он вольется в компанию, какие вопросы задает, как ведет себя. Никто не хочет общаться с замкнутым токсичным чуваком (я встречал таких). 

Могу дать несколько советов исходя из своего опыта

    -  Учитесь, учитесь всегда. И будьте инициативными. Есть люди. которые достигают некоторого уровня, переходят из одной зоны комфорта в другую и перестают развиваться. Таких людей сразу видят. Такие люди не берут сложные задачи или боятся их брать. Такие люди -середнячки. Это неплохо, но стоит всегда стремиться к лучшему уровню. Тех, у кого горит всегда огонь - их всегда замечают.
    -  Иногда бывает тяжело, в процессе изучения чего то нового и это нормально.
    -  Найдите хобби, желательно спортивное. Плавание, лазание по горам или лыжи - что угодно, что будет отвлекать вас, что не даст думать. 
    -  Окружите себя сильными ребятами. Если вы лучший среди своего окружения - возможно его стоит поменять и найти сильную команду, где вы будете самым слабым.
    -  Изучайте новые технологии, но не гонитесь за модой. Есть вещи, которые никогда не устареют - знания сетей, операционных систем, алгоритмов, общая теория программирования. Языки становятся популярными, потом пропадают. Появляются и исчезают новые фреймворки. Но базовые (фундаментальные) знания позволяют понять, как устроены многие вещи, это помогает изучать новое (за последнее время ничего не появилось прям приципиально нового).

### Как получаешь новую информацию [&uarr;](#devmap)

    - хабр
    - тематические чаты в соцсетях
    - дайджест новых технологий в ленте новостей
    - на видеохостингах интересно смотреть презентации новых технологий
    - периодически попадаются подборки рекомендованных книг к прочтению - для этой цели купил электронную книгу - чтоб техническая литература была более доступна

### что интересно по жизни, какие хобби [&uarr;](#devmap)

    - техподдержка домашнего сервера
    - возиться с железом - даже собрал свою mini itx систему
    - пешие прогулки, велопрогулки
    - постоянно нахожусь в процессе совершенствования своих знаний иностранных языков
    - готовить - подобные рутинные процессы инога помогают сменить ритм мыслей и обдумать что то важное

### какие три последние книги прочитал [&uarr;](#devmap)

    - Мэтт Зандстра: PHP 8. Объекты, шаблоны и методики программирования
    - Роберт Мартин: Идеальный программист. Как стать профессионалом разработки ПО
    - Совершенный код - С. Макконелл

### Отношение к работе с чужим кодом [&uarr;](#devmap)

Читать чужой код полезно.

Наиболее очевидным способом повысить качество своего программирования является писать больше программ. Каждый знает это. Однако другой способ, который, я уверен, улучшит ваше программирование, — совершенно противоположный.

Если вы желаете резко поднять ваше умение программировать, необходимо… читать код, написанный другими программистами.

Как читать

    1) Изучите общую картину - первично изучить структуру проекта. Зафиксировать для себя структуру файлов

    Цель этого шага состоит просто в ознакомлении с источником. Разберитесь, какие файлы включают в себя / вызывают / загружают другие файлы, где находится основная часть кода, какие пространства имён используются (если таковые имеются) и прочее такого рода. Получив общее представление, можно погружаться в детали.

    2) Документируйте ваши результаты

    Чтение кода не должно быть каким-то пассивным действием. Рекомендую добавлять комментарии по мере продвижения, документировать ваши предположения и ваши выводы, когда вы начнёте понимать ход выполнения программы.

    По мере понимания процессов вы можете удалить небольшие иерархические комментарии, которые вы оставляли для себя, и, возможно, написать более значимые и авторитетные комментарии, которые могли бы быть переданы обратно в проект.

    3) Используй тесты

    Будем надеяться, что проект, который вы выбрали, имеет набор тестов. Если нет, то можете пропустить этот раздел вообще.

    Тесты являются отличным местом для начала чтения чужого кода, потому что они документируют то, что программа должна выполнить. Одни тесты являются более информативными, чем другие, но независимо от того, насколько хорошо они написаны, часто найти намерения программиста в тестах намного легче, чем в реализации. При чтении попытайтесь получить успешный результат при прогоне всего набора тестов. Это позволит убедиться, что ваша среда разработки сконфигурирована правильно, и сделает вас более уверенным при внесении изменений.

    4) Измените код, скомпилируйте

    Вы начнёте, действительно, понимать код, только после того, как сломаете всё и снова соберёте вместе.

    5) Смыть и повторить

    Закончив с чтением одной кодовой базы, возьмите другую и начните процесс снова. Чем больше кодов вы читаете, тем лучше идёт это чтение и тем больше вы получаете из него за меньшее время. Я думаю, вы обнаружите, что ваш КПД растёт довольно быстро и что это действительно очень приятный способ обучения.


### что сделал в своей жизни такого, чем можешь гордиться [&uarr;](#devmap)

Понял свои сильные стороны - всегда нравились точные науки, и смог это начать применять - раньше учился в инженерном вузе и затем работал инженером.
Но со временем понял что имею дело с сильно устаревшими знаниями. 

Поэтому начал заниматься самообразованием и путем проб и ошибок пришел к тому,чем сейчас занимаюсь.

Горжусь тем - что смог сделать свое хобби, своей работой.

### какие задачи интересуют [&uarr;](#devmap)

ну тут зависит от вакансии на которую собеседуешься - желательно подстраивать список под стек технологий компании, с добавлением возможных смежных технологий

### Интересные проекты / задачи [&uarr;](#devmap)

- самописные модули в bitrix, laravel
- интеграции с crm - настройка самых разных типов импорта. Проектирвоание унифицированного типа импорта работающего с разными типами файлов.
- создание блочных конструкторов страниц


### Какой твой любимый язык или фреймворк? Теперь расскажи его минусы. [&uarr;](#devmap)

впринципе не важен фреймворк - это то по сути только инструмент. Важны именно парадигмы которыми ты пользуешься при разработке продукта. Даже самый объектно ориентированный фреймворк можно превратить в набор методов с кучей процедурного кода.
